{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87637d4f",
   "metadata": {},
   "source": [
    "# 00b — Setup & Data Preparation (FMA Small)\n",
    "\n",
    "Prepara il dataset FMA Small (8 generi). Usa stessi parametri audio di GTZAN (sr=22050, n_mels=128, durata segmenti ~3s).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a235271e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FMA_ROOT: /home/alepot55/Desktop/projects/naml_project/data/fma_small\n",
      "PROCESSED: /home/alepot55/Desktop/projects/naml_project/data/processed_fma\n"
     ]
    }
   ],
   "source": [
    "# Paths & placeholders\n",
    "import os, sys, json, pickle, numpy as np, subprocess, shutil, re, zipfile\n",
    "from pathlib import Path\n",
    "PROJECT_ROOT = Path(os.getcwd()).resolve().parents[1]\n",
    "FMA_ROOT = PROJECT_ROOT/'data'/'fma_small'\n",
    "PROCESSED = PROJECT_ROOT/'data'/'processed_fma'\n",
    "KAGGLE_DIR = PROJECT_ROOT/'kaggle'\n",
    "PROCESSED.mkdir(parents=True, exist_ok=True)\n",
    "print('FMA_ROOT:', FMA_ROOT)\n",
    "print('PROCESSED:', PROCESSED)\n",
    "\n",
    "# User options\n",
    "ONLY_SMALL = True   # True => scarica solo la parte \"small\" (consigliato per ridurre dimensioni)\n",
    "PREFER_KAGGLEHUB = False  # Se True, prova KaggleHub prima (potrebbe scaricare molto di più)\n",
    "\n",
    "# Official mirror URLs (FMA project)\n",
    "OFFICIAL_SMALL_URL = 'https://os.unil.cloud.switch.ch/fma/fma_small.zip'\n",
    "\n",
    "# Helpers\n",
    "\n",
    "def ensure_package(pkg_spec: str):\n",
    "    try:\n",
    "        __import__(pkg_spec.split('[')[0])\n",
    "        return True\n",
    "    except Exception:\n",
    "        try:\n",
    "            print(f'Installing {pkg_spec} ...')\n",
    "            subprocess.run([sys.executable, '-m', 'pip', 'install', pkg_spec], check=True)\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f'Failed to install {pkg_spec}:', e)\n",
    "            return False\n",
    "\n",
    "\n",
    "def setup_kaggle_api_creds():\n",
    "    kaggle_json = KAGGLE_DIR/'kaggle.json'\n",
    "    if kaggle_json.exists():\n",
    "        kaggle_home = Path.home()/'.kaggle'\n",
    "        kaggle_home.mkdir(exist_ok=True)\n",
    "        dest = kaggle_home/'kaggle.json'\n",
    "        if not dest.exists():\n",
    "            shutil.copy2(kaggle_json, dest)\n",
    "            dest.chmod(0o600)\n",
    "        return True\n",
    "    print('kaggle.json not found in', KAGGLE_DIR)\n",
    "    return False\n",
    "\n",
    "\n",
    "def kaggle_api_download_exact_file(dataset_slug: str, filename: str, dl_dir: Path) -> Path | None:\n",
    "    \"\"\"Download a specific file using the Kaggle API (Python) to avoid CLI dependency.\"\"\"\n",
    "    try:\n",
    "        if not ensure_package('kaggle'):\n",
    "            print('Cannot install kaggle package; aborting Kaggle API download.')\n",
    "            return None\n",
    "        from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "        api = KaggleApi()\n",
    "        api.authenticate()  # relies on ~/.kaggle/kaggle.json\n",
    "        print(f'Downloading {filename} from {dataset_slug} via Kaggle API ...')\n",
    "        print(f'Dataset URL: https://www.kaggle.com/datasets/{dataset_slug}')\n",
    "        api.dataset_download_file(dataset_slug, filename, path=str(dl_dir), force=True)\n",
    "        return dl_dir/filename\n",
    "    except Exception as e:\n",
    "        print(f'Kaggle API download failed for {filename}:', e)\n",
    "        return None\n",
    "\n",
    "\n",
    "def http_download(url: str, dest_path: Path) -> bool:\n",
    "    \"\"\"Stream download a file over HTTP into dest_path using requests (installed on demand).\"\"\"\n",
    "    if not ensure_package('requests'):\n",
    "        print('Cannot install requests; aborting HTTP download.')\n",
    "        return False\n",
    "    import requests\n",
    "    try:\n",
    "        with requests.get(url, stream=True, timeout=60) as r:\n",
    "            r.raise_for_status()\n",
    "            total = int(r.headers.get('content-length', 0))\n",
    "            chunk = 1024 * 1024\n",
    "            written = 0\n",
    "            with open(dest_path, 'wb') as f:\n",
    "                for data in r.iter_content(chunk_size=chunk):\n",
    "                    if data:\n",
    "                        f.write(data)\n",
    "                        written += len(data)\n",
    "                        if total:\n",
    "                            pct = 100.0 * written / total\n",
    "                            print(f\"Downloading... {written/1e6:.1f}MB/{total/1e6:.1f}MB ({pct:.1f}%)\", end='\\r')\n",
    "        print('\\nHTTP download complete:', dest_path)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print('HTTP download failed:', e)\n",
    "        return False\n",
    "\n",
    "\n",
    "def unzip_to_dir(zip_path: Path, out_dir: Path) -> bool:\n",
    "    try:\n",
    "        with zipfile.ZipFile(zip_path, 'r') as zf:\n",
    "            zf.extractall(out_dir)\n",
    "        print('Unzipped:', zip_path, '->', out_dir)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print('Unzip failed:', e)\n",
    "        return False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0282488",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FMA small already present at /home/alepot55/Desktop/projects/naml_project/data/fma_small — skipping download.\n"
     ]
    }
   ],
   "source": [
    "# Download logic (idempotent: riusa cache/zips se presenti e non riscarica)\n",
    "dl_dir = PROJECT_ROOT/'data'/'fma_download'\n",
    "dl_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Consider dataset present only if there are mp3 files available\n",
    "mp3_present = FMA_ROOT.exists() and any(FMA_ROOT.rglob('*.mp3'))\n",
    "if not mp3_present:\n",
    "    used_download = False\n",
    "\n",
    "    # Reuse previously extracted/cached content in dl_dir if available\n",
    "    cand_dirs = list(dl_dir.rglob('fma_small'))\n",
    "    small_dir = cand_dirs[0] if cand_dirs else None\n",
    "    if small_dir and any(small_dir.rglob('*.mp3')):\n",
    "        FMA_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "        for item in small_dir.iterdir():\n",
    "            target = FMA_ROOT/item.name\n",
    "            if item.is_dir():\n",
    "                shutil.copytree(item, target, dirs_exist_ok=True)\n",
    "            else:\n",
    "                shutil.copy2(item, target)\n",
    "        print('FMA small prepared from cache at', small_dir, '->', FMA_ROOT)\n",
    "        used_download = True\n",
    "\n",
    "    # Preferred: only download the \"small\" zip using Kaggle API (size-friendly)\n",
    "    if not used_download and ONLY_SMALL:\n",
    "        zip_path = dl_dir/'fma_small.zip'\n",
    "        if setup_kaggle_api_creds():\n",
    "            if zip_path.exists():\n",
    "                print('Found existing zip (Kaggle):', zip_path, '— skip re-download.')\n",
    "            else:\n",
    "                # Known archive name in the official dataset\n",
    "                kz = kaggle_api_download_exact_file('mdeff/fma', 'fma_small.zip', dl_dir)\n",
    "                if kz and kz.exists():\n",
    "                    print('Downloaded via Kaggle API:', kz)\n",
    "                    zip_path = kz\n",
    "            # Unzip if needed and prepare FMA_ROOT\n",
    "            if zip_path.exists():\n",
    "                if not small_dir or not small_dir.exists():\n",
    "                    if unzip_to_dir(zip_path, dl_dir):\n",
    "                        cand_dirs = list(dl_dir.rglob('fma_small'))\n",
    "                        small_dir = cand_dirs[0] if cand_dirs else None\n",
    "                if small_dir and small_dir.exists():\n",
    "                    FMA_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "                    for item in small_dir.iterdir():\n",
    "                        target = FMA_ROOT/item.name\n",
    "                        if item.is_dir():\n",
    "                            shutil.copytree(item, target, dirs_exist_ok=True)\n",
    "                        else:\n",
    "                            shutil.copy2(item, target)\n",
    "                    print('FMA small ready at', FMA_ROOT)\n",
    "                    used_download = True\n",
    "                else:\n",
    "                    print('Unzip complete but fma_small folder not found. Please check archive contents.')\n",
    "        else:\n",
    "            print('Kaggle credentials not configured; skipping Kaggle API path.')\n",
    "\n",
    "    # Official HTTP mirror fallback (no Kaggle account required)\n",
    "    if not used_download and ONLY_SMALL:\n",
    "        print('Trying official FMA mirror (HTTP):', OFFICIAL_SMALL_URL)\n",
    "        local_zip = dl_dir/'fma_small.zip'\n",
    "        if local_zip.exists():\n",
    "            print('Found existing zip (HTTP):', local_zip, '— skip re-download.')\n",
    "        else:\n",
    "            if not http_download(OFFICIAL_SMALL_URL, local_zip):\n",
    "                print('HTTP download failed; cannot proceed with mirror path.')\n",
    "        # Unzip if needed and prepare FMA_ROOT\n",
    "        if local_zip.exists():\n",
    "            if not small_dir or not small_dir.exists():\n",
    "                if unzip_to_dir(local_zip, dl_dir):\n",
    "                    cand_dirs = list(dl_dir.rglob('fma_small'))\n",
    "                    small_dir = cand_dirs[0] if cand_dirs else None\n",
    "            if small_dir and small_dir.exists():\n",
    "                FMA_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "                for item in small_dir.iterdir():\n",
    "                    target = FMA_ROOT/item.name\n",
    "                    if item.is_dir():\n",
    "                        shutil.copytree(item, target, dirs_exist_ok=True)\n",
    "                    else:\n",
    "                        shutil.copy2(item, target)\n",
    "                print('FMA small ready at', FMA_ROOT)\n",
    "                used_download = True\n",
    "            else:\n",
    "                print('Unzip complete but fma_small folder not found (HTTP mirror).')\n",
    "\n",
    "    # Optional KaggleHub path (may download larger content)\n",
    "    if not used_download and PREFER_KAGGLEHUB:\n",
    "        if ensure_package('kagglehub[pandas-datasets]'):\n",
    "            try:\n",
    "                import kagglehub\n",
    "                print('Attempting download via KaggleHub (imsparsh/fma-free-music-archive-small-medium) ...')\n",
    "                base_path = kagglehub.dataset_download('imsparsh/fma-free-music-archive-small-medium')\n",
    "                base_path = Path(base_path)\n",
    "                print('KaggleHub local cache:', base_path)\n",
    "                # Look for fma_small folder or zip inside downloaded dataset\n",
    "                cand_dirs = list(base_path.rglob('fma_small'))\n",
    "                small_dir = cand_dirs[0] if cand_dirs else None\n",
    "                if not small_dir:\n",
    "                    zips = list(base_path.rglob('*fma_small*.zip'))\n",
    "                    if zips:\n",
    "                        dz = zips[0]\n",
    "                        out_dir = dz.parent\n",
    "                        print('Unzipping', dz)\n",
    "                        unzip_to_dir(dz, out_dir)\n",
    "                        cand_dirs = list(out_dir.rglob('fma_small'))\n",
    "                        small_dir = cand_dirs[0] if cand_dirs else None\n",
    "                if small_dir and small_dir.exists():\n",
    "                    FMA_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "                    for item in small_dir.iterdir():\n",
    "                        target = FMA_ROOT/item.name\n",
    "                        if item.is_dir():\n",
    "                            shutil.copytree(item, target, dirs_exist_ok=True)\n",
    "                        else:\n",
    "                            shutil.copy2(item, target)\n",
    "                    print('FMA small prepared at', FMA_ROOT)\n",
    "                    used_download = True\n",
    "                else:\n",
    "                    print('KaggleHub: fma_small not found inside dataset cache.')\n",
    "            except Exception as e:\n",
    "                print('KaggleHub download attempt failed:', e)\n",
    "\n",
    "    # Final fallback: skip download and require manual placement (keeps project offline-safe)\n",
    "    if not used_download:\n",
    "        print('Automatic download not completed. Please place fma_small manually in', FMA_ROOT)\n",
    "else:\n",
    "    print('FMA small already present at', FMA_ROOT, '— skipping download.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed69cc2",
   "metadata": {},
   "source": [
    "> Nota: per mantenere il progetto offline-safe, questo notebook non scarica automaticamente FMA. Posiziona `fma_small` in `data/fma_small`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a14eccbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando metadata: /home/alepot55/Desktop/projects/naml_project/data/fma_download/fma_metadata/tracks.csv\n",
      "Classi (genre_top) rilevate: ['Electronic', 'Experimental', 'Folk', 'Hip-Hop', 'Instrumental', 'International', 'Pop', 'Rock']\n",
      "Numero campioni totali: 8000\n",
      "Train/Val/Test sizes: 4800/1600/1600\n",
      "Classi (genre_top) rilevate: ['Electronic', 'Experimental', 'Folk', 'Hip-Hop', 'Instrumental', 'International', 'Pop', 'Rock']\n",
      "Numero campioni totali: 8000\n",
      "Train/Val/Test sizes: 4800/1600/1600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 323/4800 [00:19<05:24, 13.79it/s][src/libmpg123/layer3.c:INT123_do_layer3():1878] error: part2_3_length (3328) too large for available bit count (3240)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1878] error: part2_3_length (3328) too large for available bit count (3240)\n",
      "  9%|▉         | 423/4800 [00:24<03:10, 22.95it/s][src/libmpg123/layer3.c:INT123_do_layer3():1948] error: dequantization failed!\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1948] error: dequantization failed!\n",
      " 37%|███▋      | 1797/4800 [01:32<02:21, 21.23it/s][src/libmpg123/layer3.c:INT123_do_layer3():1908] error: dequantization failed!\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1908] error: dequantization failed!\n",
      " 76%|███████▌  | 3640/4800 [03:12<00:47, 24.36it/s][src/libmpg123/layer3.c:INT123_do_layer3():1878] error: part2_3_length (3360) too large for available bit count (3240)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1878] error: part2_3_length (3360) too large for available bit count (3240)\n",
      " 84%|████████▍ | 4056/4800 [03:31<00:42, 17.54it/s]Note: Illegal Audio-MPEG-Header 0x00000000 at offset 22401.\n",
      "Note: Trying to resync...\n",
      "Note: Skipped 1024 bytes in input.\n",
      "[src/libmpg123/parse.c:wetwork():1389] error: Giving up resync after 1024 bytes - your stream is not nice... (maybe increasing resync limit could help).\n",
      " 85%|████████▍ | 4059/4800 [03:32<00:42, 17.32it/s]Note: Illegal Audio-MPEG-Header 0x00000000 at offset 22401.\n",
      "Note: Trying to resync...\n",
      "Note: Skipped 1024 bytes in input.\n",
      "[src/libmpg123/parse.c:wetwork():1389] error: Giving up resync after 1024 bytes - your stream is not nice... (maybe increasing resync limit could help).\n",
      "100%|██████████| 4800/4800 [04:08<00:00, 19.29it/s]\n",
      "100%|██████████| 4800/4800 [04:08<00:00, 19.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avviso: saltati 4794 file per errori di decodifica.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 1123/1600 [00:59<00:20, 23.55it/s][src/libmpg123/layer3.c:INT123_do_layer3():1908] error: dequantization failed!\n",
      " 70%|███████   | 1126/1600 [01:00<00:19, 24.70it/s][src/libmpg123/layer3.c:INT123_do_layer3():1908] error: dequantization failed!\n",
      " 76%|███████▋  | 1222/1600 [01:04<00:15, 23.89it/s][src/libmpg123/parse.c:do_readahead():1123] warning: Cannot read next header, a one-frame stream? Duh...\n",
      "[src/libmpg123/parse.c:do_readahead():1123] warning: Cannot read next header, a one-frame stream? Duh...\n",
      " 83%|████████▎ | 1329/1600 [01:08<00:11, 23.74it/s][src/libmpg123/layer3.c:INT123_do_layer3():1908] error: dequantization failed!\n",
      "Note: Illegal Audio-MPEG-Header 0x00000000 at offset 63168.\n",
      "Note: Trying to resync...\n",
      "Note: Skipped 1024 bytes in input.\n",
      "[src/libmpg123/parse.c:wetwork():1389] error: Giving up resync after 1024 bytes - your stream is not nice... (maybe increasing resync limit could help).\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1908] error: dequantization failed!\n",
      "Note: Illegal Audio-MPEG-Header 0x00000000 at offset 63168.\n",
      "Note: Trying to resync...\n",
      "Note: Skipped 1024 bytes in input.\n",
      "[src/libmpg123/parse.c:wetwork():1389] error: Giving up resync after 1024 bytes - your stream is not nice... (maybe increasing resync limit could help).\n",
      "100%|██████████| 1600/1600 [01:22<00:00, 19.45it/s]\n",
      "100%|██████████| 1600/1600 [01:22<00:00, 19.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avviso: saltati 1597 file per errori di decodifica.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▍        | 233/1600 [00:12<00:57, 23.79it/s]Note: Illegal Audio-MPEG-Header 0x00000000 at offset 33361.\n",
      "Note: Trying to resync...\n",
      "Note: Skipped 1024 bytes in input.\n",
      "[src/libmpg123/parse.c:wetwork():1389] error: Giving up resync after 1024 bytes - your stream is not nice... (maybe increasing resync limit could help).\n",
      "Note: Illegal Audio-MPEG-Header 0x00000000 at offset 33361.\n",
      "Note: Trying to resync...\n",
      "Note: Skipped 1024 bytes in input.\n",
      "[src/libmpg123/parse.c:wetwork():1389] error: Giving up resync after 1024 bytes - your stream is not nice... (maybe increasing resync limit could help).\n",
      " 15%|█▌        | 247/1600 [00:12<00:53, 25.38it/s][src/libmpg123/parse.c:do_readahead():1123] warning: Cannot read next header, a one-frame stream? Duh...\n",
      "[src/libmpg123/parse.c:do_readahead():1123] warning: Cannot read next header, a one-frame stream? Duh...\n",
      " 81%|████████▏ | 1301/1600 [01:04<00:13, 22.85it/s][src/libmpg123/parse.c:do_readahead():1123] warning: Cannot read next header, a one-frame stream? Duh...\n",
      " 82%|████████▏ | 1305/1600 [01:04<00:11, 25.17it/s][src/libmpg123/parse.c:do_readahead():1123] warning: Cannot read next header, a one-frame stream? Duh...\n",
      "100%|██████████| 1600/1600 [01:19<00:00, 20.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avviso: saltati 1595 file per errori di decodifica.\n",
      "Saved processed FMA arrays: (60, 128, 128, 1) (30, 128, 128, 1) (50, 128, 128, 1)\n",
      "Class mapping: {'Electronic': 0, 'Experimental': 1, 'Folk': 2, 'Hip-Hop': 3, 'Instrumental': 4, 'International': 5, 'Pop': 6, 'Rock': 7}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Processing — use FMA metadata (tracks.csv) to assign genre_top labels and avoid stratify errors\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "from contextlib import contextmanager, redirect_stderr, redirect_stdout\n",
    "\n",
    "# Try to prefer ffmpeg backend for audioread if available (reduces mpg123 noise)\n",
    "os.environ.setdefault(\"AUDIOREAD_PLUGIN\", \"ffmpeg\")\n",
    "os.environ.setdefault(\"AUDIOREAD_BACKEND\", \"ffmpeg\")\n",
    "\n",
    "@contextmanager\n",
    "def quiet_audioio():\n",
    "    \"\"\"Suppress decoder stdout/stderr and warnings (mpg123/ffmpeg chatter).\"\"\"\n",
    "    with open(os.devnull, 'w') as devnull:\n",
    "        with redirect_stderr(devnull), redirect_stdout(devnull):\n",
    "            with warnings.catch_warnings():\n",
    "                warnings.simplefilter(\"ignore\")\n",
    "                yield\n",
    "\n",
    "# Ensure dataset presence\n",
    "if not FMA_ROOT.exists():\n",
    "    raise FileNotFoundError(\n",
    "        'Dataset FMA Small non trovato. Esegui prima la prima cella per scaricare via Kaggle API o mirror, oppure posiziona fma_small in data/fma_small.'\n",
    "    )\n",
    "\n",
    "# Ensure metadata (tracks.csv) is available; try to locate or download from official mirror (idempotent)\n",
    "METADATA_ROOT = PROJECT_ROOT/'data'/'fma_metadata'\n",
    "TRACKS_CSV = METADATA_ROOT/'tracks.csv'\n",
    "OFFICIAL_METADATA_URL = 'https://os.unil.cloud.switch.ch/fma/fma_metadata.zip'\n",
    "dl_dir = PROJECT_ROOT/'data'/'fma_download'\n",
    "dl_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "if not TRACKS_CSV.exists():\n",
    "    # Try to find tracks.csv under any previously downloaded folder\n",
    "    cand = list(dl_dir.rglob('tracks.csv')) if dl_dir.exists() else []\n",
    "    if cand:\n",
    "        METADATA_ROOT = cand[0].parent\n",
    "        TRACKS_CSV = cand[0]\n",
    "    else:\n",
    "        # Attempt to reuse existing meta zip or download if missing, then unzip\n",
    "        print('tracks.csv non trovato. Cerco un fma_metadata.zip locale o scarico dal mirror ufficiale...')\n",
    "        meta_zip = dl_dir/'fma_metadata.zip'\n",
    "        if meta_zip.exists():\n",
    "            print('Trovato fma_metadata.zip in cache:', meta_zip, '— skip re-download.')\n",
    "        else:\n",
    "            if 'http_download' in globals():\n",
    "                if not http_download(OFFICIAL_METADATA_URL, meta_zip):\n",
    "                    print('Download metadati (HTTP) fallito.')\n",
    "            else:\n",
    "                print('Helper http_download non disponibile: esegui la prima cella del notebook.')\n",
    "        # Unzip only if tracks.csv still not present\n",
    "        if not any(dl_dir.rglob('tracks.csv')):\n",
    "            if 'unzip_to_dir' in globals() and meta_zip.exists():\n",
    "                if unzip_to_dir(meta_zip, dl_dir):\n",
    "                    pass\n",
    "            else:\n",
    "                print('Helper unzip_to_dir non disponibile o zip mancante; impossibile estrarre metadati.')\n",
    "        cand = list(dl_dir.rglob('tracks.csv'))\n",
    "        if cand:\n",
    "            METADATA_ROOT = cand[0].parent\n",
    "            TRACKS_CSV = cand[0]\n",
    "\n",
    "if not TRACKS_CSV.exists():\n",
    "    raise FileNotFoundError(\n",
    "        'tracks.csv non disponibile. Scarica fma_metadata.zip manualmente dal sito ufficiale o esegui la prima cella per abilitare il download.'\n",
    "    )\n",
    "\n",
    "print('Usando metadata:', TRACKS_CSV)\n",
    "\n",
    "# Load metadata; tracks.csv uses MultiIndex columns\n",
    "tracks = pd.read_csv(TRACKS_CSV, header=[0, 1], index_col=0, low_memory=False)\n",
    "# genre_top is at ('track', 'genre_top'), index is track_id\n",
    "if ('track', 'genre_top') not in tracks.columns:\n",
    "    raise RuntimeError('Colonna (track, genre_top) non trovata in tracks.csv: versione metadati inattesa.')\n",
    "\n",
    "id_to_genre = tracks[('track', 'genre_top')]\n",
    "\n",
    "# Gather mp3 files and map to genre_top via track id (filename stem)\n",
    "mp3_files = sorted([p for p in FMA_ROOT.rglob('*.mp3')])\n",
    "if len(mp3_files) == 0:\n",
    "    raise RuntimeError('Nessun file .mp3 trovato in fma_small.')\n",
    "\n",
    "files, labels = [], []\n",
    "for p in mp3_files:\n",
    "    try:\n",
    "        tid = int(p.stem)\n",
    "    except Exception:\n",
    "        # Unexpected filename, skip\n",
    "        continue\n",
    "    lab = id_to_genre.get(tid)\n",
    "    if pd.isna(lab):\n",
    "        continue\n",
    "    files.append(str(p))\n",
    "    labels.append(str(lab))\n",
    "\n",
    "if len(files) == 0:\n",
    "    raise RuntimeError('Nessun file etichettato trovato: controlla che tracks.csv corrisponda a fma_small.')\n",
    "\n",
    "# Filter out rare classes with < 2 samples to satisfy stratify\n",
    "from collections import Counter\n",
    "cnt = Counter(labels)\n",
    "kept_classes = {c for c, n in cnt.items() if n >= 2}\n",
    "if len(kept_classes) < len(cnt):\n",
    "    dropped = sorted([c for c, n in cnt.items() if n < 2])\n",
    "    print(f'Avviso: escludo {len(dropped)} classi con <2 campioni (per stratify):', dropped[:10], '...')\n",
    "\n",
    "files, labels = zip(*[(f, y) for f, y in zip(files, labels) if y in kept_classes])\n",
    "files, labels = list(files), list(labels)\n",
    "\n",
    "unique_classes = sorted(set(labels))\n",
    "print('Classi (genre_top) rilevate:', unique_classes)\n",
    "print('Numero campioni totali:', len(labels))\n",
    "\n",
    "# Determine whether stratify is possible\n",
    "can_stratify = True\n",
    "if len(unique_classes) < 2:\n",
    "    can_stratify = False\n",
    "else:\n",
    "    min_count = min(Counter(labels).values())\n",
    "    if min_count < 2:\n",
    "        can_stratify = False\n",
    "\n",
    "strat_arg = labels if can_stratify else None\n",
    "if not can_stratify:\n",
    "    print('Stratify disattivato (classi insufficienti o troppo sbilanciate). Userò uno split standard.')\n",
    "\n",
    "train_val_files, test_files, train_val_labels, test_labels = train_test_split(\n",
    "    files, labels, test_size=0.2, random_state=42, stratify=strat_arg\n",
    ")\n",
    "\n",
    "strat_arg_tv = train_val_labels if can_stratify else None\n",
    "train_files, val_files, train_labels, val_labels = train_test_split(\n",
    "    train_val_files, train_val_labels, test_size=0.25, random_state=42, stratify=strat_arg_tv\n",
    ")\n",
    "\n",
    "print(f'Train/Val/Test sizes: {len(train_files)}/{len(val_files)}/{len(test_files)}')\n",
    "\n",
    "# Feature extraction (mirror of GTZAN settings)\n",
    "def extract(files, labels, sr=22050, n_mels=128, hop_length=512, seg=2.97, n_segments=10):\n",
    "    X, y = [], []\n",
    "    seg_len = int(sr * seg)\n",
    "    errors = 0\n",
    "    for fp, lab in tqdm(list(zip(files, labels))):\n",
    "        try:\n",
    "            with quiet_audioio():\n",
    "                ysig, _ = librosa.load(fp, sr=sr, mono=True, res_type='kaiser_fast')\n",
    "            for s in range(n_segments):\n",
    "                st, en = s * seg_len, s * seg_len + seg_len\n",
    "                if en <= len(ysig):\n",
    "                    mel = librosa.feature.melspectrogram(y=ysig[st:en], sr=sr, n_mels=n_mels, hop_length=hop_length)\n",
    "                    X.append(librosa.power_to_db(mel, ref=np.max))\n",
    "                    y.append(lab)\n",
    "        except Exception:\n",
    "            errors += 1\n",
    "            continue\n",
    "    if errors:\n",
    "        print(f'Avviso: saltati {errors} file per errori di decodifica.')\n",
    "    return X, y\n",
    "\n",
    "Xtr_list, ytr_txt = extract(train_files, train_labels)\n",
    "Xva_list, yva_txt = extract(val_files, val_labels)\n",
    "Xte_list, yte_txt = extract(test_files, test_labels)\n",
    "\n",
    "# Unify time dimension to T=128 (pad/crop)\n",
    "def unify(lst, T=128):\n",
    "    out = []\n",
    "    for s in lst:\n",
    "        if s.shape[1] > T:\n",
    "            out.append(s[:, :T])\n",
    "        else:\n",
    "            out.append(np.pad(s, ((0, 0), (0, T - s.shape[1])), 'constant'))\n",
    "    return np.array(out)\n",
    "\n",
    "Xtr, Xva, Xte = unify(Xtr_list), unify(Xva_list), unify(Xte_list)\n",
    "\n",
    "# Scale features (fit on train only)\n",
    "scaler = StandardScaler()\n",
    "\n",
    "def fit_transform_3d(X, fit=False):\n",
    "    sh = X.shape\n",
    "    Z = X.reshape(sh[0], -1)\n",
    "    Z = scaler.fit_transform(Z) if fit else scaler.transform(Z)\n",
    "    return Z.reshape(sh)\n",
    "\n",
    "Xtr = fit_transform_3d(Xtr, fit=True)\n",
    "Xva = fit_transform_3d(Xva)\n",
    "Xte = fit_transform_3d(Xte)\n",
    "\n",
    "# Add channel dimension\n",
    "Xtr = Xtr[..., None]\n",
    "Xva = Xva[..., None]\n",
    "Xte = Xte[..., None]\n",
    "\n",
    "# Encode labels\n",
    "le = LabelEncoder().fit(unique_classes)\n",
    "ytr = le.transform(ytr_txt)\n",
    "yva = le.transform(yva_txt)\n",
    "yte = le.transform(yte_txt)\n",
    "\n",
    "# Persist arrays and transformers\n",
    "np.save(PROCESSED/'X_train.npy', Xtr)\n",
    "np.save(PROCESSED/'y_train.npy', ytr)\n",
    "np.save(PROCESSED/'X_val.npy', Xva)\n",
    "np.save(PROCESSED/'y_val.npy', yva)\n",
    "np.save(PROCESSED/'X_test.npy', Xte)\n",
    "np.save(PROCESSED/'y_test.npy', yte)\n",
    "with open(PROCESSED/'label_encoder.pkl', 'wb') as f:\n",
    "    pickle.dump(le, f)\n",
    "with open(PROCESSED/'scaler.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "\n",
    "print('Saved processed FMA arrays:', Xtr.shape, Xva.shape, Xte.shape)\n",
    "print('Class mapping:', dict(zip(le.classes_.tolist(), range(len(le.classes_)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ba37e0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tracks.csv path candidate: /home/alepot55/Desktop/projects/naml_project/data/fma_download/fma_metadata/tracks.csv\n",
      "tracks.csv loaded. Shape: (106574, 52)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"10\" halign=\"left\">album</th>\n",
       "      <th>...</th>\n",
       "      <th colspan=\"10\" halign=\"left\">track</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>comments</th>\n",
       "      <th>date_created</th>\n",
       "      <th>date_released</th>\n",
       "      <th>engineer</th>\n",
       "      <th>favorites</th>\n",
       "      <th>id</th>\n",
       "      <th>information</th>\n",
       "      <th>listens</th>\n",
       "      <th>producer</th>\n",
       "      <th>tags</th>\n",
       "      <th>...</th>\n",
       "      <th>information</th>\n",
       "      <th>interest</th>\n",
       "      <th>language_code</th>\n",
       "      <th>license</th>\n",
       "      <th>listens</th>\n",
       "      <th>lyricist</th>\n",
       "      <th>number</th>\n",
       "      <th>publisher</th>\n",
       "      <th>tags</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>track_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2008-11-26 01:44:45</td>\n",
       "      <td>2009-01-05 00:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;p&gt;&lt;/p&gt;</td>\n",
       "      <td>6073</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4656</td>\n",
       "      <td>en</td>\n",
       "      <td>Attribution-NonCommercial-ShareAlike 3.0 Inter...</td>\n",
       "      <td>1293</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>Food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>2008-11-26 01:44:45</td>\n",
       "      <td>2009-01-05 00:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;p&gt;&lt;/p&gt;</td>\n",
       "      <td>6073</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1470</td>\n",
       "      <td>en</td>\n",
       "      <td>Attribution-NonCommercial-ShareAlike 3.0 Inter...</td>\n",
       "      <td>514</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>Electric Ave</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>2008-11-26 01:44:45</td>\n",
       "      <td>2009-01-05 00:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;p&gt;&lt;/p&gt;</td>\n",
       "      <td>6073</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1933</td>\n",
       "      <td>en</td>\n",
       "      <td>Attribution-NonCommercial-ShareAlike 3.0 Inter...</td>\n",
       "      <td>1151</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>This World</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>2008-11-26 01:45:08</td>\n",
       "      <td>2008-02-06 00:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>47632</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>54881</td>\n",
       "      <td>en</td>\n",
       "      <td>Attribution-NonCommercial-NoDerivatives (aka M...</td>\n",
       "      <td>50135</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>Freeway</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0</td>\n",
       "      <td>2008-11-26 01:45:05</td>\n",
       "      <td>2009-01-06 00:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>&lt;p&gt; \"spiritual songs\" from Nicky Cook&lt;/p&gt;</td>\n",
       "      <td>2710</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>978</td>\n",
       "      <td>en</td>\n",
       "      <td>Attribution-NonCommercial-NoDerivatives (aka M...</td>\n",
       "      <td>361</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>Spiritual Level</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 52 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            album                                                     \\\n",
       "         comments         date_created        date_released engineer   \n",
       "track_id                                                               \n",
       "2               0  2008-11-26 01:44:45  2009-01-05 00:00:00      NaN   \n",
       "3               0  2008-11-26 01:44:45  2009-01-05 00:00:00      NaN   \n",
       "5               0  2008-11-26 01:44:45  2009-01-05 00:00:00      NaN   \n",
       "10              0  2008-11-26 01:45:08  2008-02-06 00:00:00      NaN   \n",
       "20              0  2008-11-26 01:45:05  2009-01-06 00:00:00      NaN   \n",
       "\n",
       "                                                                          \\\n",
       "         favorites id                                information listens   \n",
       "track_id                                                                   \n",
       "2                4  1                                    <p></p>    6073   \n",
       "3                4  1                                    <p></p>    6073   \n",
       "5                4  1                                    <p></p>    6073   \n",
       "10               4  6                                        NaN   47632   \n",
       "20               2  4  <p> \"spiritual songs\" from Nicky Cook</p>    2710   \n",
       "\n",
       "                        ...       track                         \\\n",
       "         producer tags  ... information interest language_code   \n",
       "track_id                ...                                      \n",
       "2             NaN   []  ...         NaN     4656            en   \n",
       "3             NaN   []  ...         NaN     1470            en   \n",
       "5             NaN   []  ...         NaN     1933            en   \n",
       "10            NaN   []  ...         NaN    54881            en   \n",
       "20            NaN   []  ...         NaN      978            en   \n",
       "\n",
       "                                                                              \\\n",
       "                                                    license listens lyricist   \n",
       "track_id                                                                       \n",
       "2         Attribution-NonCommercial-ShareAlike 3.0 Inter...    1293      NaN   \n",
       "3         Attribution-NonCommercial-ShareAlike 3.0 Inter...     514      NaN   \n",
       "5         Attribution-NonCommercial-ShareAlike 3.0 Inter...    1151      NaN   \n",
       "10        Attribution-NonCommercial-NoDerivatives (aka M...   50135      NaN   \n",
       "20        Attribution-NonCommercial-NoDerivatives (aka M...     361      NaN   \n",
       "\n",
       "                                                 \n",
       "         number publisher tags            title  \n",
       "track_id                                         \n",
       "2             3       NaN   []             Food  \n",
       "3             4       NaN   []     Electric Ave  \n",
       "5             6       NaN   []       This World  \n",
       "10            1       NaN   []          Freeway  \n",
       "20            3       NaN   []  Spiritual Level  \n",
       "\n",
       "[5 rows x 52 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Opzionale: anteprima metadati locale (tracks.csv) ed esempio KaggleHub disabilitato per evitare 404/deprecations\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Resolve PROJECT_ROOT if missing (in case cell 1 wasn't run)\n",
    "try:\n",
    "    PROJECT_ROOT\n",
    "except NameError:\n",
    "    from pathlib import Path\n",
    "    PROJECT_ROOT = Path(os.getcwd()).resolve().parents[1]\n",
    "\n",
    "# Try to use TRACKS_CSV from previous cell; otherwise, fall back to default paths/search\n",
    "try:\n",
    "    TRACKS_CSV\n",
    "except NameError:\n",
    "    METADATA_ROOT = PROJECT_ROOT/'data'/'fma_metadata'\n",
    "    TRACKS_CSV = METADATA_ROOT/'tracks.csv'\n",
    "    if not TRACKS_CSV.exists():\n",
    "        dl_dir = PROJECT_ROOT/'data'/'fma_download'\n",
    "        cand = list(dl_dir.rglob('tracks.csv')) if dl_dir.exists() else []\n",
    "        if cand:\n",
    "            TRACKS_CSV = cand[0]\n",
    "\n",
    "print('tracks.csv path candidate:', TRACKS_CSV)\n",
    "if Path(TRACKS_CSV).exists():\n",
    "    df_tracks = pd.read_csv(TRACKS_CSV, header=[0,1], index_col=0, low_memory=False)\n",
    "    print('tracks.csv loaded. Shape:', df_tracks.shape)\n",
    "    display(df_tracks.head())\n",
    "else:\n",
    "    print('tracks.csv non trovato. Esegui le prime celle per scaricare i metadati, oppure posiziona fma_metadata/ in data/.')\n",
    "\n",
    "# Facoltativo: esempio KaggleHub (disabilitato di default)\n",
    "USE_KAGGLEHUB = False  # imposta a True solo se sai esattamente cosa stai caricando\n",
    "DATASET_ID = \"mdeff/fma\"   # dataset ufficiale; i CSV sono in fma_metadata.zip, non direttamente accessibili via Pandas adapter\n",
    "FILE_PATH = \"tracks.csv\"   # nome del file da leggere (se disponibile via adapter)\n",
    "\n",
    "if USE_KAGGLEHUB:\n",
    "    try:\n",
    "        import kagglehub\n",
    "        # Nota: kagglehub.load_dataset (Pandas adapter) è deprecato e può dare 404 se il file non è esposto dal dataset\n",
    "        # Percorso consigliato: scaricare l'archivio con kagglehub.dataset_download e poi leggere localmente\n",
    "        base_path = kagglehub.dataset_download(DATASET_ID)\n",
    "        base_path = Path(base_path)\n",
    "        print('KaggleHub cache:', base_path)\n",
    "        # Se presente, prova a trovare tracks.csv o estrarre fma_metadata.zip\n",
    "        cand = list(base_path.rglob('tracks.csv'))\n",
    "        if not cand:\n",
    "            zips = list(base_path.rglob('fma_metadata*.zip'))\n",
    "            if zips:\n",
    "                import zipfile\n",
    "                with zipfile.ZipFile(zips[0], 'r') as zf:\n",
    "                    zf.extractall(zips[0].parent)\n",
    "                cand = list(base_path.rglob('tracks.csv'))\n",
    "        if cand:\n",
    "            df_kh = pd.read_csv(cand[0], header=[0,1], index_col=0, low_memory=False)\n",
    "            print('KaggleHub tracks.csv loaded. Shape:', df_kh.shape)\n",
    "            display(df_kh.head())\n",
    "        else:\n",
    "            print('KaggleHub: tracks.csv non trovato nel dataset scaricato.')\n",
    "    except Exception as e:\n",
    "        print('KaggleHub preview failed:', e)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
