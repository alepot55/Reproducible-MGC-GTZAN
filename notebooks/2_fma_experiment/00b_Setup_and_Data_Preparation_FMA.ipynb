{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87637d4f",
   "metadata": {},
   "source": [
    "# 00b — Setup & Data Preparation (FMA Small)\n",
    "\n",
    "Prepara il dataset FMA Small (8 generi). Usa stessi parametri audio di GTZAN (sr=22050, n_mels=128, durata segmenti ~3s).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebbc021f",
   "metadata": {},
   "source": [
    "## Panoramica\n",
    "\n",
    "Questo notebook prepara FMA Small in 5 step chiari:\n",
    "\n",
    "1. Configurazione e logging (parametri, caps, parallelismo)\n",
    "2. Download/Verifica integrità (mirror ufficiale + check decodifica)\n",
    "3. Preparazione/Feature extraction (ffmpeg, segmenti ~3s, padding robusto)\n",
    "4. Bilanciamento per classe e split riproducibili\n",
    "5. Salvataggio array + report riassuntivo\n",
    "\n",
    "Suggerimenti:\n",
    "\n",
    "- Per corse veloci: riduci MAX_TRACKS_PER_CLASS e/o FFMPEG_MAX_SECS.\n",
    "- Per output puliti: tutti i messaggi usano il logger con timestamp.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a235271e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FMA_ROOT: /home/alepot55/Desktop/projects/naml_project/data/fma_small\n",
      "PROCESSED: /home/alepot55/Desktop/projects/naml_project/data/processed_fma\n",
      "Installing imageio-ffmpeg ...\n",
      "Requirement already satisfied: imageio-ffmpeg in /home/alepot55/Desktop/projects/naml_project/venv/lib/python3.12/site-packages (0.6.0)\n",
      "FFmpeg configured from imageio-ffmpeg at: /home/alepot55/Desktop/projects/naml_project/venv/lib/python3.12/site-packages/imageio_ffmpeg/binaries/ffmpeg-linux-x86_64-v7.0.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Paths & placeholders\n",
    "import os, sys, json, pickle, numpy as np, subprocess, shutil, re, zipfile\n",
    "from pathlib import Path\n",
    "PROJECT_ROOT = Path(os.getcwd()).resolve().parents[1]\n",
    "FMA_ROOT = PROJECT_ROOT/'data'/'fma_small'\n",
    "PROCESSED = PROJECT_ROOT/'data'/'processed_fma'\n",
    "KAGGLE_DIR = PROJECT_ROOT/'kaggle'\n",
    "PROCESSED.mkdir(parents=True, exist_ok=True)\n",
    "print('FMA_ROOT:', FMA_ROOT)\n",
    "print('PROCESSED:', PROCESSED)\n",
    "\n",
    "# User options\n",
    "ONLY_SMALL = True   # True => scarica solo la parte \"small\" (consigliato per ridurre dimensioni)\n",
    "PREFER_KAGGLEHUB = False  # Se True, prova KaggleHub prima (potrebbe scaricare molto di più)\n",
    "\n",
    "# Official mirror URLs (FMA project)\n",
    "OFFICIAL_SMALL_URL = 'https://os.unil.cloud.switch.ch/fma/fma_small.zip'\n",
    "\n",
    "# Helpers\n",
    "\n",
    "def ensure_package(pkg_spec: str):\n",
    "    try:\n",
    "        __import__(pkg_spec.split('[')[0])\n",
    "        return True\n",
    "    except Exception:\n",
    "        try:\n",
    "            print(f'Installing {pkg_spec} ...')\n",
    "            subprocess.run([sys.executable, '-m', 'pip', 'install', pkg_spec], check=True)\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f'Failed to install {pkg_spec}:', e)\n",
    "            return False\n",
    "\n",
    "# Prefer ffmpeg-backed decoding via imageio-ffmpeg (bundled binary)\n",
    "try:\n",
    "    if ensure_package('imageio-ffmpeg'):\n",
    "        import imageio_ffmpeg\n",
    "        ffexe = imageio_ffmpeg.get_ffmpeg_exe()\n",
    "        ffdir = str(Path(ffexe).parent)\n",
    "        os.environ['PATH'] = ffdir + os.pathsep + os.environ.get('PATH', '')\n",
    "        os.environ.setdefault('AUDIOREAD_BACKEND', 'ffmpeg')\n",
    "        os.environ.setdefault('AUDIOREAD_PLUGIN', 'ffmpeg')\n",
    "        print('FFmpeg configured from imageio-ffmpeg at:', ffexe)\n",
    "except Exception as e:\n",
    "    print('FFmpeg setup warning:', e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393e2206",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Indian Music Genre dataset acquisition via KaggleHub\n",
    "import os, shutil\n",
    "from pathlib import Path\n",
    "import kagglehub\n",
    "\n",
    "\n",
    "\n",
    "INDIAN_DATASET_ID = os.environ.get('INDIAN_DATASET_ID', 'winchester19/indian-music-genre-dataset')\n",
    "INDIAN_ROOT = PROJECT_ROOT / 'data' / 'indian_music'\n",
    "\n",
    "\n",
    "def ensure_indian_dataset():\n",
    "    INDIAN_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "    log(f'Trying KaggleHub dataset_download({INDIAN_DATASET_ID})')\n",
    "    try:\n",
    "        path = kagglehub.dataset_download(INDIAN_DATASET_ID)\n",
    "        log(f'KaggleHub path: {path}')\n",
    "        # Copy/rsync-like: move audio folders into INDIAN_ROOT (idempotent)\n",
    "        src = Path(path)\n",
    "        # The dataset has top-level genre folders; copy them if not present\n",
    "        copied = 0\n",
    "        for item in src.iterdir():\n",
    "            if item.is_dir():\n",
    "                dest = INDIAN_ROOT / item.name\n",
    "                if not dest.exists():\n",
    "                    shutil.copytree(item, dest)\n",
    "                    copied += 1\n",
    "        log(f'Indian dataset prepared at {INDIAN_ROOT} (copied {copied} dirs).')\n",
    "        return INDIAN_ROOT\n",
    "    except Exception as e:\n",
    "        log(f'Indian dataset download failed: {e}', level='WARN')\n",
    "        return None\n",
    "\n",
    "\n",
    "# Uncomment to fetch when needed:\n",
    "\n",
    "\n",
    "# ensure_indian_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cde5e6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logging utility for clean, consistent messages\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "VERBOSE = os.environ.get('FMA_VERBOSE', '1') == '1'\n",
    "\n",
    "def log(msg: str, level: str = 'INFO'):\n",
    "    \"\"\"Print a timestamped log line. Set FMA_VERBOSE=0 to reduce noise.\"\"\"\n",
    "    if not VERBOSE and level == 'INFO':\n",
    "        return\n",
    "    ts = datetime.now().strftime('%H:%M:%S')\n",
    "    print(f'[{ts}] {level}: {msg}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39c9724c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: SR= 22050 N_MELS= 128 HOP= 512 SEG_DUR= 2.97 N_SEGMENTS= 10\n",
      "Caps/Speed: MAX_TRACKS_PER_CLASS= 40 FFMPEG_MAX_SECS= 30.0 DECODE_BACKEND= auto PARALLEL= False N_JOBS= 10\n",
      "Derived: T_TARGET= 129\n"
     ]
    }
   ],
   "source": [
    "# Configuration — fast, reproducible, and controllable\n",
    "import os\n",
    "from multiprocessing import cpu_count\n",
    "\n",
    "# Audio params (mirror GTZAN defaults)\n",
    "SR = int(os.environ.get('FMA_SR', '22050'))\n",
    "N_MELS = int(os.environ.get('FMA_N_MELS', '128'))\n",
    "HOP = int(os.environ.get('FMA_HOP', '512'))\n",
    "SEG_DUR = float(os.environ.get('FMA_SEG_DUR', '2.97'))  # seconds\n",
    "N_SEGMENTS = int(os.environ.get('FMA_N_SEGMENTS', '10'))\n",
    "FFMPEG_MAX_SECS = float(os.environ.get('FMA_FFMPEG_MAX_SECS', '30'))  # decode at most 30s per track\n",
    "\n",
    "# Decoding backend: 'auto' (try librosa, fallback ffmpeg), 'librosa', or 'ffmpeg'\n",
    "DECODE_BACKEND = os.environ.get('FMA_DECODE_BACKEND', 'auto').lower()\n",
    "\n",
    "# Speed/size controls\n",
    "MAX_TRACKS_PER_CLASS = int(os.environ.get('FMA_MAX_TRACKS_PER_CLASS', '40'))  # cap for speed/memory; set -1 for all\n",
    "PARALLEL = os.environ.get('FMA_PARALLEL', '0') == '1'  # set 1 to enable joblib parallel extraction\n",
    "N_JOBS = int(os.environ.get('FMA_PARALLEL_JOBS', str(max(1, cpu_count() // 2))))\n",
    "\n",
    "# Cache behavior\n",
    "FORCE_REDO = os.environ.get('FMA_FORCE_REDO', '0') == '1'  # if 1, ignore cached processed arrays\n",
    "TRUST_CACHE = os.environ.get('FMA_TRUST_CACHE', '1') == '1'  # if 1, trust meta match and skip validation\n",
    "\n",
    "# Derived target time frames (≈130 for 2.97s at hop=512, sr=22050)\n",
    "T_TARGET = int(round(SEG_DUR * SR / HOP)) + 1\n",
    "\n",
    "CONFIG = {\n",
    "    'SR': SR,\n",
    "    'N_MELS': N_MELS,\n",
    "    'HOP': HOP,\n",
    "    'SEG_DUR': SEG_DUR,\n",
    "    'N_SEGMENTS': N_SEGMENTS,\n",
    "    'FFMPEG_MAX_SECS': FFMPEG_MAX_SECS,\n",
    "    'DECODE_BACKEND': DECODE_BACKEND,\n",
    "    'MAX_TRACKS_PER_CLASS': MAX_TRACKS_PER_CLASS,\n",
    "    'PARALLEL': PARALLEL,\n",
    "    'N_JOBS': N_JOBS,\n",
    "    'T_TARGET': T_TARGET,\n",
    "}\n",
    "\n",
    "print('Config: SR=', SR, 'N_MELS=', N_MELS, 'HOP=', HOP, 'SEG_DUR=', SEG_DUR, 'N_SEGMENTS=', N_SEGMENTS)\n",
    "print('Caps/Speed: MAX_TRACKS_PER_CLASS=', MAX_TRACKS_PER_CLASS, 'FFMPEG_MAX_SECS=', FFMPEG_MAX_SECS, 'DECODE_BACKEND=', DECODE_BACKEND, 'PARALLEL=', PARALLEL, 'N_JOBS=', N_JOBS)\n",
    "print('Derived: T_TARGET=', T_TARGET)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0282488",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:41:47] INFO: Decode check sample: 0/120 ok (0.0%)\n",
      "[17:41:47] WARN: High fraction of decode failures, considering re-download...\n",
      "Download already complete (31961630661 bytes).\n",
      "Extracting files...\n",
      "[17:45:50] INFO: KaggleHub cache: /home/alepot55/.cache/kagglehub/datasets/imsparsh/fma-free-music-archive-small-medium/versions/1\n",
      "[17:45:50] INFO: Found fma_small in KaggleHub cache: /home/alepot55/.cache/kagglehub/datasets/imsparsh/fma-free-music-archive-small-medium/versions/1/fma_small\n",
      "[17:45:50] INFO: Previous fma_small backed up to /home/alepot55/Desktop/projects/naml_project/data/fma_small_backup_1756050350\n",
      "[17:46:09] INFO: FMA small ready at /home/alepot55/Desktop/projects/naml_project/data/fma_small (from KaggleHub)\n",
      "[17:46:10] INFO: Sample file sizes (bytes): [962041, 1202384, 480908, 1202286, 961286, 962035, 720635, 1201253, 1201198, 1202123]\n",
      "[17:46:10] INFO: ffmpeg probe ok count: 10/10\n"
     ]
    }
   ],
   "source": [
    "# Download logic (KaggleHub-first; fall back to official mirror; idempotent)\n",
    "import random, time, shutil\n",
    "from pathlib import Path\n",
    "\n",
    "# Ensure ffmpeg-backed audioread here too (defensive, in case cell 1 not run)\n",
    "os.environ.setdefault('AUDIOREAD_BACKEND', 'ffmpeg')\n",
    "os.environ.setdefault('AUDIOREAD_PLUGIN', 'ffmpeg')\n",
    "\n",
    "# Self-contained helpers\n",
    "import urllib.request\n",
    "\n",
    "def http_download(url: str, dest: Path, chunk_size: int = 1 << 20) -> bool:\n",
    "    try:\n",
    "        dest.parent.mkdir(parents=True, exist_ok=True)\n",
    "        with urllib.request.urlopen(url) as r, open(dest, 'wb') as f:\n",
    "            while True:\n",
    "                chunk = r.read(chunk_size)\n",
    "                if not chunk:\n",
    "                    break\n",
    "                f.write(chunk)\n",
    "        log(f'Downloaded {url} -> {dest}')\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        log(f'HTTP download failed: {e}', level='WARN')\n",
    "        return False\n",
    "\n",
    "import zipfile\n",
    "\n",
    "def unzip_to_dir(zip_path: Path, out_dir: Path) -> bool:\n",
    "    try:\n",
    "        with zipfile.ZipFile(zip_path, 'r') as zf:\n",
    "            zf.extractall(out_dir)\n",
    "        log(f'Unzipped {zip_path} -> {out_dir}')\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        log(f'Unzip failed: {e}', level='WARN')\n",
    "        return False\n",
    "\n",
    "# Lightweight ffmpeg probe to validate decode-ability\n",
    "import subprocess as sp\n",
    "\n",
    "def ffmpeg_probe_ok(path: Path) -> bool:\n",
    "    try:\n",
    "        r = sp.run(['ffmpeg', '-v', 'error', '-i', str(path), '-f', 'null', '-'], stdout=sp.PIPE, stderr=sp.PIPE)\n",
    "        return r.returncode == 0\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "import librosa\n",
    "\n",
    "def quick_decode_check(mp3_paths, sample_size=200, sr=22050, secs=1.0):\n",
    "    mp3_list = list(mp3_paths)\n",
    "    if not mp3_list:\n",
    "        return 0, 0\n",
    "    k = min(sample_size, len(mp3_list))\n",
    "    sampled = random.sample(mp3_list, k)\n",
    "    ok = 0\n",
    "    for p in sampled:\n",
    "        try:\n",
    "            y, _ = librosa.load(str(p), sr=sr, mono=True, duration=secs, res_type='kaiser_fast')\n",
    "            if y is not None and y.size > 0:\n",
    "                ok += 1\n",
    "        except Exception:\n",
    "            continue\n",
    "    return ok, k\n",
    "\n",
    "# Paths\n",
    "dl_dir = PROJECT_ROOT/'data'/'fma_download'\n",
    "dl_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Consider dataset present only if there are mp3 files available\n",
    "mp3_present = FMA_ROOT.exists() and any(FMA_ROOT.rglob('*.mp3'))\n",
    "needs_redownload = False\n",
    "\n",
    "if mp3_present:\n",
    "    mp3_files_all = list(FMA_ROOT.rglob('*.mp3'))\n",
    "    ok, k = quick_decode_check(mp3_files_all, sample_size=120)\n",
    "    ratio = (ok / k) if k else 0.0\n",
    "    log(f\"Decode check sample: {ok}/{k} ok ({ratio*100:.1f}%)\")\n",
    "    if ratio < 0.5:\n",
    "        log('High fraction of decode failures, considering re-download...', level='WARN')\n",
    "        needs_redownload = True\n",
    "else:\n",
    "    needs_redownload = True\n",
    "\n",
    "used_download = False\n",
    "small_dir = None\n",
    "\n",
    "# Prefer KaggleHub (requested). Controlled by FMA_KAGGLEHUB=1 (default 1).\n",
    "USE_KAGGLEHUB = os.environ.get('FMA_KAGGLEHUB', '1') == '1'\n",
    "KAGGLEHUB_DATASET_ID = 'imsparsh/fma-free-music-archive-small-medium'\n",
    "\n",
    "if needs_redownload and USE_KAGGLEHUB:\n",
    "    try:\n",
    "        try:\n",
    "            import kagglehub  # type: ignore\n",
    "        except Exception:\n",
    "            if 'ensure_package' in globals():\n",
    "                ensure_package('kagglehub')\n",
    "                import kagglehub  # type: ignore\n",
    "        base_path = Path(kagglehub.dataset_download(KAGGLEHUB_DATASET_ID))\n",
    "        log(f'KaggleHub cache: {base_path}')\n",
    "        # Locate fma_small inside the dataset cache\n",
    "        cand_dirs = [p for p in base_path.rglob('fma_small') if p.is_dir()]\n",
    "        if not cand_dirs:\n",
    "            zips = list(base_path.rglob('fma_small*.zip'))\n",
    "            if zips:\n",
    "                unzip_to_dir(zips[0], zips[0].parent)\n",
    "                cand_dirs = [p for p in base_path.rglob('fma_small') if p.is_dir()]\n",
    "        if cand_dirs:\n",
    "            small_dir = cand_dirs[0]\n",
    "            log(f'Found fma_small in KaggleHub cache: {small_dir}')\n",
    "            if FMA_ROOT.exists():\n",
    "                backup = FMA_ROOT.with_name(f\"fma_small_backup_{int(time.time())}\")\n",
    "                shutil.move(str(FMA_ROOT), str(backup))\n",
    "                log(f'Previous fma_small backed up to {backup}', level='INFO')\n",
    "            FMA_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "            for item in Path(small_dir).iterdir():\n",
    "                target = FMA_ROOT/item.name\n",
    "                if item.is_dir():\n",
    "                    shutil.copytree(item, target, dirs_exist_ok=True)\n",
    "                else:\n",
    "                    shutil.copy2(item, target)\n",
    "            log(f'FMA small ready at {FMA_ROOT} (from KaggleHub)')\n",
    "            used_download = True\n",
    "        else:\n",
    "            log('KaggleHub: fma_small not found inside dataset cache.', level='WARN')\n",
    "    except Exception as e:\n",
    "        log(f'KaggleHub download failed: {e}', level='WARN')\n",
    "\n",
    "# Fallback to official mirror if KaggleHub not used or failed\n",
    "if needs_redownload and not used_download:\n",
    "    local_zip = dl_dir/'fma_small.zip'\n",
    "    if local_zip.exists():\n",
    "        log(f'Found existing archive: {local_zip} — reusing')\n",
    "    else:\n",
    "        log(f'Trying official FMA mirror: {OFFICIAL_SMALL_URL}')\n",
    "        if not http_download(OFFICIAL_SMALL_URL, local_zip):\n",
    "            log('Mirror download failed; cannot proceed automatically.', level='WARN')\n",
    "    if local_zip.exists():\n",
    "        if unzip_to_dir(local_zip, dl_dir):\n",
    "            cand_dirs = list(dl_dir.rglob('fma_small'))\n",
    "            small_dir = cand_dirs[0] if cand_dirs else None\n",
    "        if small_dir and Path(small_dir).exists():\n",
    "            if FMA_ROOT.exists():\n",
    "                backup = FMA_ROOT.with_name(f\"fma_small_backup_{int(time.time())}\")\n",
    "                shutil.move(str(FMA_ROOT), str(backup))\n",
    "                log(f'Previous fma_small backed up to {backup}', level='INFO')\n",
    "            FMA_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "            for item in Path(small_dir).iterdir():\n",
    "                target = FMA_ROOT/item.name\n",
    "                if item.is_dir():\n",
    "                    shutil.copytree(item, target, dirs_exist_ok=True)\n",
    "                else:\n",
    "                    shutil.copy2(item, target)\n",
    "            log(f'FMA small ready at {FMA_ROOT}')\n",
    "            used_download = True\n",
    "        else:\n",
    "            log('Unzip complete but fma_small folder not found in archive.', level='WARN')\n",
    "\n",
    "if not used_download and mp3_present and not needs_redownload:\n",
    "    log('FMA small present and decode check passed — skipping download.')\n",
    "\n",
    "# Post-check a tiny sample to diagnose failures without re-downloading again\n",
    "try:\n",
    "    sample_files = list(FMA_ROOT.rglob('*.mp3'))\n",
    "    if sample_files:\n",
    "        sample_files = random.sample(sample_files, min(10, len(sample_files)))\n",
    "        sizes = [Path(p).stat().st_size for p in sample_files]\n",
    "        probes = [ffmpeg_probe_ok(Path(p)) for p in sample_files]\n",
    "        log(f'Sample file sizes (bytes): {sizes}')\n",
    "        log(f'ffmpeg probe ok count: {sum(probes)}/{len(probes)}')\n",
    "        if sum(probes) == 0:\n",
    "            log('ffmpeg cannot decode sampled files. Archive may be corrupted. Set FMA_FORCE_REDO=1 to force a fresh download.', level='WARN')\n",
    "except Exception as e:\n",
    "    log(f'Post-check diagnostics failed: {e}', level='WARN')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed69cc2",
   "metadata": {},
   "source": [
    "> Nota: per mantenere il progetto offline-safe, questo notebook non scarica automaticamente FMA. Posiziona `fma_small` in `data/fma_small`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a14eccbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:46:10] INFO: Usando metadata: /home/alepot55/Desktop/projects/naml_project/data/fma_download/fma_metadata/tracks.csv\n",
      "[17:46:12] INFO: Applied cap per class: 40\n",
      "[17:46:12] INFO: Classes: ['Electronic', 'Experimental', 'Folk', 'Hip-Hop', 'Instrumental', 'International', 'Pop', 'Rock'] | Total tracks considered: 320\n",
      "[17:46:12] INFO: Stratify enabled: True\n",
      "[17:46:12] INFO: Train/Val/Test sizes: 192/64/64\n",
      "[17:46:12] INFO: Applied cap per class: 40\n",
      "[17:46:12] INFO: Classes: ['Electronic', 'Experimental', 'Folk', 'Hip-Hop', 'Instrumental', 'International', 'Pop', 'Rock'] | Total tracks considered: 320\n",
      "[17:46:12] INFO: Stratify enabled: True\n",
      "[17:46:12] INFO: Train/Val/Test sizes: 192/64/64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 192/192 [00:50<00:00,  3.79it/s]\n",
      "100%|██████████| 192/192 [00:50<00:00,  3.79it/s]\n",
      "100%|██████████| 64/64 [00:16<00:00,  3.78it/s]\n",
      "100%|██████████| 64/64 [00:16<00:00,  3.78it/s]\n",
      "100%|██████████| 64/64 [00:17<00:00,  3.73it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:47:38] INFO: Saved processed FMA arrays: (1920, 128, 129, 1), (640, 128, 129, 1), (640, 128, 129, 1)\n",
      "[17:47:38] INFO: Class mapping: {'Electronic': 0, 'Experimental': 1, 'Folk': 2, 'Hip-Hop': 3, 'Instrumental': 4, 'International': 5, 'Pop': 6, 'Rock': 7}\n"
     ]
    }
   ],
   "source": [
    "# Processing — metadata mapping, balanced split, caching, and extraction\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "from contextlib import contextmanager, redirect_stderr, redirect_stdout\n",
    "\n",
    "# Prefer ffmpeg backend for audioread if available\n",
    "os.environ.setdefault(\"AUDIOREAD_PLUGIN\", \"ffmpeg\")\n",
    "os.environ.setdefault(\"AUDIOREAD_BACKEND\", \"ffmpeg\")\n",
    "\n",
    "@contextmanager\n",
    "def quiet_audioio():\n",
    "    \"\"\"Suppress decoder stdout/stderr and warnings (mpg123/ffmpeg chatter).\"\"\"\n",
    "    with open(os.devnull, 'w') as devnull:\n",
    "        with redirect_stderr(devnull), redirect_stdout(devnull):\n",
    "            with warnings.catch_warnings():\n",
    "                warnings.simplefilter(\"ignore\")\n",
    "                yield\n",
    "\n",
    "# Cache metadata file to avoid recomputation\n",
    "META_JSON = PROCESSED/'fma_prep_meta.json'\n",
    "REQUIRED_FILES = [PROCESSED/f for f in ['X_train.npy','y_train.npy','X_val.npy','y_val.npy','X_test.npy','y_test.npy','label_encoder.pkl','scaler.pkl']]\n",
    "\n",
    "# Early exit if cache exists and matches current config\n",
    "if not FORCE_REDO and all(p.exists() for p in REQUIRED_FILES) and META_JSON.exists():\n",
    "    try:\n",
    "        with open(META_JSON,'r') as f:\n",
    "            meta = json.load(f)\n",
    "        cfg = meta.get('config', {})\n",
    "        cfg_match = all(str(cfg.get(k)) == str(v) for k, v in CONFIG.items())\n",
    "        if TRUST_CACHE and cfg_match:\n",
    "            log('Processed FMA artifacts already present and config matches. Skipping extraction.', level='INFO')\n",
    "            log(f\"X shapes: {np.load(PROCESSED/'X_train.npy').shape}, {np.load(PROCESSED/'X_val.npy').shape}, {np.load(PROCESSED/'X_test.npy').shape}\")\n",
    "            raise SystemExit\n",
    "        else:\n",
    "            log('Cache exists but config differs (or TRUST_CACHE=0). Proceeding to recompute.', level='WARN')\n",
    "    except Exception:\n",
    "        log('Cache check failed; proceeding to compute.', level='WARN')\n",
    "\n",
    "# Ensure dataset presence\n",
    "if not FMA_ROOT.exists():\n",
    "    raise FileNotFoundError(\n",
    "        'Dataset FMA Small non trovato. Esegui la prima cella (download) oppure posiziona fma_small in data/fma_small.'\n",
    "    )\n",
    "\n",
    "# Ensure metadata (tracks.csv)\n",
    "METADATA_ROOT = PROJECT_ROOT/'data'/'fma_metadata'\n",
    "TRACKS_CSV = METADATA_ROOT/'tracks.csv'\n",
    "OFFICIAL_METADATA_URL = 'https://os.unil.cloud.switch.ch/fma/fma_metadata.zip'\n",
    "dl_dir = PROJECT_ROOT/'data'/'fma_download'\n",
    "dl_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "if not TRACKS_CSV.exists():\n",
    "    cand = list(dl_dir.rglob('tracks.csv')) if dl_dir.exists() else []\n",
    "    if cand:\n",
    "        METADATA_ROOT = cand[0].parent\n",
    "        TRACKS_CSV = cand[0]\n",
    "    else:\n",
    "        log('tracks.csv non trovato. Provo a scaricare fma_metadata.zip (mirror ufficiale)...', level='INFO')\n",
    "        meta_zip = dl_dir/'fma_metadata.zip'\n",
    "        if not meta_zip.exists():\n",
    "            if 'http_download' in globals():\n",
    "                http_download(OFFICIAL_METADATA_URL, meta_zip)\n",
    "        if meta_zip.exists():\n",
    "            if 'unzip_to_dir' in globals():\n",
    "                unzip_to_dir(meta_zip, dl_dir)\n",
    "            cand = list(dl_dir.rglob('tracks.csv'))\n",
    "            if cand:\n",
    "                METADATA_ROOT = cand[0].parent\n",
    "                TRACKS_CSV = cand[0]\n",
    "\n",
    "if not TRACKS_CSV.exists():\n",
    "    raise FileNotFoundError('tracks.csv non disponibile. Scarica fma_metadata.zip manualmente o abilita il download.')\n",
    "\n",
    "log(f'Usando metadata: {TRACKS_CSV}')\n",
    "\n",
    "# Load metadata; tracks.csv uses MultiIndex columns\n",
    "tracks = pd.read_csv(TRACKS_CSV, header=[0, 1], index_col=0, low_memory=False)\n",
    "if ('track', 'genre_top') not in tracks.columns:\n",
    "    raise RuntimeError('Colonna (track, genre_top) non trovata in tracks.csv: versione metadati inattesa.')\n",
    "\n",
    "id_to_genre = tracks[('track', 'genre_top')]\n",
    "\n",
    "# Gather mp3 files and map to genre_top via track id (filename stem)\n",
    "mp3_files = sorted([p for p in FMA_ROOT.rglob('*.mp3')])\n",
    "if len(mp3_files) == 0:\n",
    "    raise RuntimeError('Nessun file .mp3 trovato in fma_small.')\n",
    "\n",
    "files, labels = [], []\n",
    "for p in mp3_files:\n",
    "    try:\n",
    "        tid = int(p.stem)\n",
    "    except Exception:\n",
    "        continue\n",
    "    lab = id_to_genre.get(tid)\n",
    "    if pd.isna(lab):\n",
    "        continue\n",
    "    files.append(str(p))\n",
    "    labels.append(str(lab))\n",
    "\n",
    "if len(files) == 0:\n",
    "    raise RuntimeError('Nessun file etichettato trovato: controlla che tracks.csv corrisponda a fma_small.')\n",
    "\n",
    "# Balanced sampling per class (optional speed-up)\n",
    "from collections import defaultdict\n",
    "pairs = list(zip(files, labels))\n",
    "by_class = defaultdict(list)\n",
    "for f, y in pairs:\n",
    "    by_class[y].append(f)\n",
    "\n",
    "if MAX_TRACKS_PER_CLASS > 0:\n",
    "    import random\n",
    "    capped_files, capped_labels = [], []\n",
    "    rng = random.Random(42)\n",
    "    for y, lst in by_class.items():\n",
    "        lst_sorted = sorted(lst)\n",
    "        rng.shuffle(lst_sorted)\n",
    "        take = lst_sorted[:MAX_TRACKS_PER_CLASS]\n",
    "        capped_files.extend(take)\n",
    "        capped_labels.extend([y] * len(take))\n",
    "    log(f'Applied cap per class: {MAX_TRACKS_PER_CLASS}')\n",
    "    files, labels = capped_files, capped_labels\n",
    "else:\n",
    "    log('No per-class cap applied.')\n",
    "\n",
    "unique_classes = sorted(set(labels))\n",
    "from collections import Counter\n",
    "cnt = Counter(labels)\n",
    "can_stratify = len(unique_classes) >= 2 and min(cnt.values()) >= 2\n",
    "log(f\"Classes: {unique_classes} | Total tracks considered: {len(labels)}\")\n",
    "log(f'Stratify enabled: {can_stratify}')\n",
    "\n",
    "strat_arg = labels if can_stratify else None\n",
    "if not can_stratify:\n",
    "    log('Stratify disattivato (classi insufficienti o troppo sbilanciate). Userò uno split standard).', level='WARN')\n",
    "\n",
    "train_val_files, test_files, train_val_labels, test_labels = train_test_split(\n",
    "    files, labels, test_size=0.2, random_state=42, stratify=strat_arg\n",
    ")\n",
    "\n",
    "strat_arg_tv = train_val_labels if can_stratify else None\n",
    "train_files, val_files, train_labels, val_labels = train_test_split(\n",
    "    train_val_files, train_val_labels, test_size=0.25, random_state=42, stratify=strat_arg_tv\n",
    ")\n",
    "\n",
    "log(f'Train/Val/Test sizes: {len(train_files)}/{len(val_files)}/{len(test_files)}')\n",
    "\n",
    "# Decoders\n",
    "import subprocess as sp\n",
    "\n",
    "def load_audio_librosa(fp: str, sr: int, max_secs: float):\n",
    "    with quiet_audioio():\n",
    "        y, _ = librosa.load(fp, sr=sr, mono=True, duration=max_secs, res_type='kaiser_fast')\n",
    "    return y\n",
    "\n",
    "def load_audio_ffmpeg(fp: str, sr: int, max_secs: float):\n",
    "    try:\n",
    "        # Decode to raw PCM via ffmpeg and read as float32\n",
    "        cmd = ['ffmpeg', '-v', 'error', '-i', fp, '-f', 'f32le', '-acodec', 'pcm_f32le', '-ac', '1', '-ar', str(sr), '-t', str(max_secs), 'pipe:1']\n",
    "        p = sp.run(cmd, stdout=sp.PIPE, stderr=sp.PIPE, check=False)\n",
    "        if p.returncode != 0 or not p.stdout:\n",
    "            return None\n",
    "        y = np.frombuffer(p.stdout, dtype=np.float32)\n",
    "        return y\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def load_audio(fp: str, sr: int, max_secs: float):\n",
    "    if DECODE_BACKEND == 'librosa':\n",
    "        return load_audio_librosa(fp, sr, max_secs)\n",
    "    if DECODE_BACKEND == 'ffmpeg':\n",
    "        return load_audio_ffmpeg(fp, sr, max_secs)\n",
    "    # auto: try librosa then fallback to ffmpeg\n",
    "    y = None\n",
    "    try:\n",
    "        y = load_audio_librosa(fp, sr, max_secs)\n",
    "    except Exception:\n",
    "        y = None\n",
    "    if y is None or (isinstance(y, np.ndarray) and y.size == 0):\n",
    "        y = load_audio_ffmpeg(fp, sr, max_secs)\n",
    "    return y\n",
    "\n",
    "# Feature extraction helpers\n",
    "def extract_one(args):\n",
    "    fp, lab = args\n",
    "    try:\n",
    "        ysig = load_audio(fp, SR, FFMPEG_MAX_SECS)\n",
    "        if ysig is None or len(ysig) == 0:\n",
    "            return None\n",
    "        seg_len = int(SR * SEG_DUR)\n",
    "        out = []\n",
    "        total = len(ysig)\n",
    "        for s in range(N_SEGMENTS):\n",
    "            st, en = s * seg_len, s * seg_len + seg_len\n",
    "            if st >= total:\n",
    "                break\n",
    "            seg_sig = ysig[st:en]\n",
    "            if len(seg_sig) < seg_len:\n",
    "                pad = np.zeros(seg_len - len(seg_sig), dtype=seg_sig.dtype)\n",
    "                seg_sig = np.concatenate([seg_sig, pad])\n",
    "            mel = librosa.feature.melspectrogram(y=seg_sig, sr=SR, n_mels=N_MELS, hop_length=HOP)\n",
    "            out.append(librosa.power_to_db(mel, ref=np.max))\n",
    "        return (out, [lab] * len(out)) if out else None\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "# Extraction driver (parallel optional)\n",
    "def extract(files, labels):\n",
    "    from itertools import chain\n",
    "    pairs = list(zip(files, labels))\n",
    "    results = []\n",
    "    if PARALLEL and len(pairs) > 1:\n",
    "        try:\n",
    "            from joblib import Parallel, delayed\n",
    "            results = Parallel(n_jobs=N_JOBS, prefer='threads')(delayed(extract_one)(p) for p in tqdm(pairs))\n",
    "        except Exception as e:\n",
    "            log(f'Parallel extract fallback to serial due to: {e}', level='WARN')\n",
    "            results = [extract_one(p) for p in tqdm(pairs)]\n",
    "    else:\n",
    "        results = [extract_one(p) for p in tqdm(pairs)]\n",
    "\n",
    "    X, y = [], []\n",
    "    for res in results:\n",
    "        if res is None:\n",
    "            continue\n",
    "        out, labs = res\n",
    "        X.extend(out)\n",
    "        y.extend(labs)\n",
    "    return X, y\n",
    "\n",
    "Xtr_list, ytr_txt = extract(train_files, train_labels)\n",
    "Xva_list, yva_txt = extract(val_files, val_labels)\n",
    "Xte_list, yte_txt = extract(test_files, test_labels)\n",
    "\n",
    "# Unify time dimension to T=T_TARGET (pad/crop)\n",
    "def unify(lst, T=T_TARGET):\n",
    "    out = []\n",
    "    for s in lst:\n",
    "        if s.shape[1] > T:\n",
    "            out.append(s[:, :T])\n",
    "        else:\n",
    "            out.append(np.pad(s, ((0, 0), (0, T - s.shape[1])), 'constant'))\n",
    "    return np.array(out, dtype=np.float32)\n",
    "\n",
    "Xtr, Xva, Xte = unify(Xtr_list), unify(Xva_list), unify(Xte_list)\n",
    "\n",
    "# Scale features (fit on train only)\n",
    "scaler = StandardScaler()\n",
    "\n",
    "def fit_transform_3d(X, fit=False):\n",
    "    sh = X.shape\n",
    "    Z = X.reshape(sh[0], -1)\n",
    "    Z = scaler.fit_transform(Z) if fit else scaler.transform(Z)\n",
    "    return Z.reshape(sh).astype(np.float32)\n",
    "\n",
    "Xtr = fit_transform_3d(Xtr, fit=True)\n",
    "Xva = fit_transform_3d(Xva)\n",
    "Xte = fit_transform_3d(Xte)\n",
    "\n",
    "# Add channel dimension\n",
    "Xtr = Xtr[..., None]\n",
    "Xva = Xva[..., None]\n",
    "Xte = Xte[..., None]\n",
    "\n",
    "# Encode labels\n",
    "le = LabelEncoder().fit(sorted(set(labels)))\n",
    "ytr = le.transform(ytr_txt)\n",
    "yva = le.transform(yva_txt)\n",
    "yte = le.transform(yte_txt)\n",
    "\n",
    "# Persist arrays and transformers\n",
    "np.save(PROCESSED/'X_train.npy', Xtr)\n",
    "np.save(PROCESSED/'y_train.npy', ytr)\n",
    "np.save(PROCESSED/'X_val.npy', Xva)\n",
    "np.save(PROCESSED/'y_val.npy', yva)\n",
    "np.save(PROCESSED/'X_test.npy', Xte)\n",
    "np.save(PROCESSED/'y_test.npy', yte)\n",
    "with open(PROCESSED/'label_encoder.pkl', 'wb') as f:\n",
    "    pickle.dump(le, f)\n",
    "with open(PROCESSED/'scaler.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "\n",
    "# Save meta for cache validation\n",
    "meta = {\n",
    "    'config': CONFIG,\n",
    "    'classes': le.classes_.tolist(),\n",
    "    'shapes': {\n",
    "        'X_train': list(Xtr.shape), 'X_val': list(Xva.shape), 'X_test': list(Xte.shape)\n",
    "    }\n",
    "}\n",
    "with open(META_JSON, 'w') as f:\n",
    "    json.dump(meta, f)\n",
    "\n",
    "log(f\"Saved processed FMA arrays: {Xtr.shape}, {Xva.shape}, {Xte.shape}\")\n",
    "log(f\"Class mapping: {dict(zip(le.classes_.tolist(), range(len(le.classes_))))}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8ad3ba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed arrays saved in: /home/alepot55/Desktop/projects/naml_project/data/processed_fma\n",
      "X shapes: (1920, 128, 129, 1) (640, 128, 129, 1) (640, 128, 129, 1)\n",
      "Train counts: {np.str_('Electronic'): 240, np.str_('Experimental'): 240, np.str_('Folk'): 240, np.str_('Hip-Hop'): 240, np.str_('Instrumental'): 240, np.str_('International'): 240, np.str_('Pop'): 240, np.str_('Rock'): 240}\n",
      "Val counts: {np.str_('Electronic'): 80, np.str_('Experimental'): 80, np.str_('Folk'): 80, np.str_('Hip-Hop'): 80, np.str_('Instrumental'): 80, np.str_('International'): 80, np.str_('Pop'): 80, np.str_('Rock'): 80}\n",
      "Test counts: {np.str_('Electronic'): 80, np.str_('Experimental'): 80, np.str_('Folk'): 80, np.str_('Hip-Hop'): 80, np.str_('Instrumental'): 80, np.str_('International'): 80, np.str_('Pop'): 80, np.str_('Rock'): 80}\n"
     ]
    }
   ],
   "source": [
    "# Final quick summary and verification (fast; uses cache if present)\n",
    "import numpy as np, pandas as pd, pickle\n",
    "from collections import Counter\n",
    "\n",
    "print('Processed arrays saved in:', PROCESSED)\n",
    "print('X shapes:', np.load(PROCESSED/'X_train.npy', mmap_mode='r').shape, np.load(PROCESSED/'X_val.npy', mmap_mode='r').shape, np.load(PROCESSED/'X_test.npy', mmap_mode='r').shape)\n",
    "\n",
    "ytr = np.load(PROCESSED/'y_train.npy', mmap_mode='r'); yva = np.load(PROCESSED/'y_val.npy', mmap_mode='r'); yte = np.load(PROCESSED/'y_test.npy', mmap_mode='r')\n",
    "with open(PROCESSED/'label_encoder.pkl','rb') as f:\n",
    "    le = pickle.load(f)\n",
    "\n",
    "idx_to_name = {i: cls for i, cls in enumerate(le.classes_)}\n",
    "\n",
    "def fmt_counts(arr):\n",
    "    cnt = Counter(np.asarray(arr).astype(int).tolist())\n",
    "    return {idx_to_name[k]: int(v) for k, v in sorted(cnt.items())}\n",
    "\n",
    "print('Train counts:', fmt_counts(ytr))\n",
    "print('Val counts:', fmt_counts(yva))\n",
    "print('Test counts:', fmt_counts(yte))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ba37e0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tracks.csv path candidate: /home/alepot55/Desktop/projects/naml_project/data/fma_download/fma_metadata/tracks.csv\n",
      "tracks.csv loaded. Shape: (106574, 52)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"10\" halign=\"left\">album</th>\n",
       "      <th>...</th>\n",
       "      <th colspan=\"10\" halign=\"left\">track</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>comments</th>\n",
       "      <th>date_created</th>\n",
       "      <th>date_released</th>\n",
       "      <th>engineer</th>\n",
       "      <th>favorites</th>\n",
       "      <th>id</th>\n",
       "      <th>information</th>\n",
       "      <th>listens</th>\n",
       "      <th>producer</th>\n",
       "      <th>tags</th>\n",
       "      <th>...</th>\n",
       "      <th>information</th>\n",
       "      <th>interest</th>\n",
       "      <th>language_code</th>\n",
       "      <th>license</th>\n",
       "      <th>listens</th>\n",
       "      <th>lyricist</th>\n",
       "      <th>number</th>\n",
       "      <th>publisher</th>\n",
       "      <th>tags</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>track_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2008-11-26 01:44:45</td>\n",
       "      <td>2009-01-05 00:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;p&gt;&lt;/p&gt;</td>\n",
       "      <td>6073</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4656</td>\n",
       "      <td>en</td>\n",
       "      <td>Attribution-NonCommercial-ShareAlike 3.0 Inter...</td>\n",
       "      <td>1293</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>Food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>2008-11-26 01:44:45</td>\n",
       "      <td>2009-01-05 00:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;p&gt;&lt;/p&gt;</td>\n",
       "      <td>6073</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1470</td>\n",
       "      <td>en</td>\n",
       "      <td>Attribution-NonCommercial-ShareAlike 3.0 Inter...</td>\n",
       "      <td>514</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>Electric Ave</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>2008-11-26 01:44:45</td>\n",
       "      <td>2009-01-05 00:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;p&gt;&lt;/p&gt;</td>\n",
       "      <td>6073</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1933</td>\n",
       "      <td>en</td>\n",
       "      <td>Attribution-NonCommercial-ShareAlike 3.0 Inter...</td>\n",
       "      <td>1151</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>This World</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>2008-11-26 01:45:08</td>\n",
       "      <td>2008-02-06 00:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>47632</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>54881</td>\n",
       "      <td>en</td>\n",
       "      <td>Attribution-NonCommercial-NoDerivatives (aka M...</td>\n",
       "      <td>50135</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>Freeway</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0</td>\n",
       "      <td>2008-11-26 01:45:05</td>\n",
       "      <td>2009-01-06 00:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>&lt;p&gt; \"spiritual songs\" from Nicky Cook&lt;/p&gt;</td>\n",
       "      <td>2710</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>978</td>\n",
       "      <td>en</td>\n",
       "      <td>Attribution-NonCommercial-NoDerivatives (aka M...</td>\n",
       "      <td>361</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>Spiritual Level</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 52 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            album                                                     \\\n",
       "         comments         date_created        date_released engineer   \n",
       "track_id                                                               \n",
       "2               0  2008-11-26 01:44:45  2009-01-05 00:00:00      NaN   \n",
       "3               0  2008-11-26 01:44:45  2009-01-05 00:00:00      NaN   \n",
       "5               0  2008-11-26 01:44:45  2009-01-05 00:00:00      NaN   \n",
       "10              0  2008-11-26 01:45:08  2008-02-06 00:00:00      NaN   \n",
       "20              0  2008-11-26 01:45:05  2009-01-06 00:00:00      NaN   \n",
       "\n",
       "                                                                          \\\n",
       "         favorites id                                information listens   \n",
       "track_id                                                                   \n",
       "2                4  1                                    <p></p>    6073   \n",
       "3                4  1                                    <p></p>    6073   \n",
       "5                4  1                                    <p></p>    6073   \n",
       "10               4  6                                        NaN   47632   \n",
       "20               2  4  <p> \"spiritual songs\" from Nicky Cook</p>    2710   \n",
       "\n",
       "                        ...       track                         \\\n",
       "         producer tags  ... information interest language_code   \n",
       "track_id                ...                                      \n",
       "2             NaN   []  ...         NaN     4656            en   \n",
       "3             NaN   []  ...         NaN     1470            en   \n",
       "5             NaN   []  ...         NaN     1933            en   \n",
       "10            NaN   []  ...         NaN    54881            en   \n",
       "20            NaN   []  ...         NaN      978            en   \n",
       "\n",
       "                                                                              \\\n",
       "                                                    license listens lyricist   \n",
       "track_id                                                                       \n",
       "2         Attribution-NonCommercial-ShareAlike 3.0 Inter...    1293      NaN   \n",
       "3         Attribution-NonCommercial-ShareAlike 3.0 Inter...     514      NaN   \n",
       "5         Attribution-NonCommercial-ShareAlike 3.0 Inter...    1151      NaN   \n",
       "10        Attribution-NonCommercial-NoDerivatives (aka M...   50135      NaN   \n",
       "20        Attribution-NonCommercial-NoDerivatives (aka M...     361      NaN   \n",
       "\n",
       "                                                 \n",
       "         number publisher tags            title  \n",
       "track_id                                         \n",
       "2             3       NaN   []             Food  \n",
       "3             4       NaN   []     Electric Ave  \n",
       "5             6       NaN   []       This World  \n",
       "10            1       NaN   []          Freeway  \n",
       "20            3       NaN   []  Spiritual Level  \n",
       "\n",
       "[5 rows x 52 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Opzionale: anteprima metadati locale (tracks.csv) ed esempio KaggleHub disabilitato per evitare 404/deprecations\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Resolve PROJECT_ROOT if missing (in case cell 1 wasn't run)\n",
    "try:\n",
    "    PROJECT_ROOT\n",
    "except NameError:\n",
    "    from pathlib import Path\n",
    "    PROJECT_ROOT = Path(os.getcwd()).resolve().parents[1]\n",
    "\n",
    "# Try to use TRACKS_CSV from previous cell; otherwise, fall back to default paths/search\n",
    "try:\n",
    "    TRACKS_CSV\n",
    "except NameError:\n",
    "    METADATA_ROOT = PROJECT_ROOT/'data'/'fma_metadata'\n",
    "    TRACKS_CSV = METADATA_ROOT/'tracks.csv'\n",
    "    if not TRACKS_CSV.exists():\n",
    "        dl_dir = PROJECT_ROOT/'data'/'fma_download'\n",
    "        cand = list(dl_dir.rglob('tracks.csv')) if dl_dir.exists() else []\n",
    "        if cand:\n",
    "            TRACKS_CSV = cand[0]\n",
    "\n",
    "print('tracks.csv path candidate:', TRACKS_CSV)\n",
    "if Path(TRACKS_CSV).exists():\n",
    "    df_tracks = pd.read_csv(TRACKS_CSV, header=[0,1], index_col=0, low_memory=False)\n",
    "    print('tracks.csv loaded. Shape:', df_tracks.shape)\n",
    "    display(df_tracks.head())\n",
    "else:\n",
    "    print('tracks.csv non trovato. Esegui le prime celle per scaricare i metadati, oppure posiziona fma_metadata/ in data/.')\n",
    "\n",
    "# Facoltativo: esempio KaggleHub (disabilitato di default)\n",
    "USE_KAGGLEHUB = False  # imposta a True solo se sai esattamente cosa stai caricando\n",
    "DATASET_ID = \"mdeff/fma\"   # dataset ufficiale; i CSV sono in fma_metadata.zip, non direttamente accessibili via Pandas adapter\n",
    "FILE_PATH = \"tracks.csv\"   # nome del file da leggere (se disponibile via adapter)\n",
    "\n",
    "if USE_KAGGLEHUB:\n",
    "    try:\n",
    "        import kagglehub\n",
    "        # Nota: kagglehub.load_dataset (Pandas adapter) è deprecato e può dare 404 se il file non è esposto dal dataset\n",
    "        # Percorso consigliato: scaricare l'archivio con kagglehub.dataset_download e poi leggere localmente\n",
    "        base_path = kagglehub.dataset_download(DATASET_ID)\n",
    "        base_path = Path(base_path)\n",
    "        print('KaggleHub cache:', base_path)\n",
    "        # Se presente, prova a trovare tracks.csv o estrarre fma_metadata.zip\n",
    "        cand = list(base_path.rglob('tracks.csv'))\n",
    "        if not cand:\n",
    "            zips = list(base_path.rglob('fma_metadata*.zip'))\n",
    "            if zips:\n",
    "                import zipfile\n",
    "                with zipfile.ZipFile(zips[0], 'r') as zf:\n",
    "                    zf.extractall(zips[0].parent)\n",
    "                cand = list(base_path.rglob('tracks.csv'))\n",
    "        if cand:\n",
    "            df_kh = pd.read_csv(cand[0], header=[0,1], index_col=0, low_memory=False)\n",
    "            print('KaggleHub tracks.csv loaded. Shape:', df_kh.shape)\n",
    "            display(df_kh.head())\n",
    "        else:\n",
    "            print('KaggleHub: tracks.csv non trovato nel dataset scaricato.')\n",
    "    except Exception as e:\n",
    "        print('KaggleHub preview failed:', e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf1df7a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FMA_ROOT exists: True\n",
      "MP3 files found: 8000\n",
      "TRACKS_CSV: /home/alepot55/Desktop/projects/naml_project/data/fma_download/fma_metadata/tracks.csv | exists: True\n",
      "Unique track ids in mp3s: 8000\n",
      "Labeled tracks with non-NaN genre_top: 8000\n",
      "Top 10 genre_top counts among labeled:\n",
      "(track, genre_top)\n",
      "Hip-Hop          1000\n",
      "Pop              1000\n",
      "Folk             1000\n",
      "Experimental     1000\n",
      "Rock             1000\n",
      "International    1000\n",
      "Electronic       1000\n",
      "Instrumental     1000\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Quick diagnostics: check mp3 coverage and labels without extracting features\n",
    "from pathlib import Path\n",
    "import os, pandas as pd\n",
    "\n",
    "# Resolve roots\n",
    "try:\n",
    "    PROJECT_ROOT\n",
    "except NameError:\n",
    "    from pathlib import Path\n",
    "    PROJECT_ROOT = Path(os.getcwd()).resolve().parents[1]\n",
    "FMA_ROOT = PROJECT_ROOT/'data'/'fma_small'\n",
    "METADATA_ROOT = PROJECT_ROOT/'data'/'fma_metadata'\n",
    "TRACKS_CSV = METADATA_ROOT/'tracks.csv'\n",
    "\n",
    "print('FMA_ROOT exists:', FMA_ROOT.exists())\n",
    "mp3_files = sorted([p for p in FMA_ROOT.rglob('*.mp3')]) if FMA_ROOT.exists() else []\n",
    "print('MP3 files found:', len(mp3_files))\n",
    "\n",
    "if not TRACKS_CSV.exists():\n",
    "    # Try find tracks.csv in download cache\n",
    "    dl_dir = PROJECT_ROOT/'data'/'fma_download'\n",
    "    cand = list(dl_dir.rglob('tracks.csv')) if dl_dir.exists() else []\n",
    "    if cand:\n",
    "        TRACKS_CSV = cand[0]\n",
    "\n",
    "print('TRACKS_CSV:', TRACKS_CSV, '| exists:', TRACKS_CSV.exists())\n",
    "\n",
    "if TRACKS_CSV.exists() and mp3_files:\n",
    "    tracks = pd.read_csv(TRACKS_CSV, header=[0,1], index_col=0, low_memory=False)\n",
    "    if ('track','genre_top') not in tracks.columns:\n",
    "        raise RuntimeError('tracks.csv missing (track, genre_top) column')\n",
    "    id_to_genre = tracks[('track','genre_top')]\n",
    "\n",
    "    def parse_tid(p: Path):\n",
    "        try:\n",
    "            return int(p.stem)\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "    tids = [parse_tid(p) for p in mp3_files]\n",
    "    tids = [t for t in tids if t is not None]\n",
    "    labeled = id_to_genre.loc[id_to_genre.index.intersection(tids)]\n",
    "    non_na = labeled.dropna()\n",
    "\n",
    "    print('Unique track ids in mp3s:', len(set(tids)))\n",
    "    print('Labeled tracks with non-NaN genre_top:', non_na.shape[0])\n",
    "    print('Top 10 genre_top counts among labeled:')\n",
    "    print(non_na.value_counts().head(10))\n",
    "else:\n",
    "    print('Skipping label diagnostics: missing mp3s or tracks.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
