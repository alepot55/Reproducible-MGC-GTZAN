{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dde20a35",
   "metadata": {},
   "source": [
    "# Music Genre Classification: A Numerical Analysis\n",
    "**Course:** Numerical Analysis for Machine Learning  \n",
    "**Project:** Critical Implementation and Analysis of Deep Learning Models for Audio Classification\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Project Objective\n",
    "\n",
    "This project provides a rigorous and reproducible implementation for classifying music genres from audio files. We critically analyze and extend the methodologies proposed in the reference paper by Patil et al. (2023), focusing on:\n",
    "1.  **Numerical Stability:** Comparing Gabor filter theory with practical spectrogram features.\n",
    "2.  **Optimization Analysis:** Evaluating the convergence and performance of Adam, SGD, and RMSprop.\n",
    "3.  **Architectural Trade-offs:** Comparing a complex U-Net-like model against a standard CNN baseline.\n",
    "4.  **Methodological Rigor:** Implementing proper data splitting and evaluation to address data leakage and produce reliable results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e170436",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-14 00:04:43.557015: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-07-14 00:04:43.558391: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-07-14 00:04:43.563751: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-07-14 00:04:43.575054: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1752444283.593219    9419 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1752444283.598955    9419 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1752444283.614823    9419 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1752444283.614863    9419 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1752444283.614865    9419 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1752444283.614867    9419 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-07-14 00:04:43.621276: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ðŸŽµ MUSIC GENRE CLASSIFICATION - ENVIRONMENT SETUP\n",
      "================================================================================\n",
      "âŒ NESSUNA GPU TROVATA. L'allenamento sarÃ  su CPU.\n",
      "âœ… Politica di Mixed Precision impostata su: mixed_float16\n",
      "âœ… RiproducibilitÃ  assicurata con RANDOM_STATE = 42\n",
      "âœ… Ambiente e configurazione pronti.\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-14 00:04:45.302950: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    }
   ],
   "source": [
    "# ===================================================================\n",
    "# CELLA 1: SETUP, IMPORTS E CONFIGURAZIONE GLOBALE\n",
    "# ===================================================================\n",
    "\n",
    "# --- Core Libraries ---\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import librosa\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# --- Scikit-Learn ---\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "# --- TensorFlow ---\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers, callbacks\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# --- Environment Configuration ---\n",
    "print(\"=\"*80)\n",
    "print(\"ðŸŽµ MUSIC GENRE CLASSIFICATION - ENVIRONMENT SETUP\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# 1. GPU Verification\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(f\"âœ… GPU(s) Trovata/e: {[tf.config.experimental.get_device_details(g)['device_name'] for g in gpus]}\")\n",
    "    except RuntimeError as e:\n",
    "        print(f\"âš ï¸ Errore durante l'inizializzazione della GPU: {e}\")\n",
    "else:\n",
    "    print(\"âŒ NESSUNA GPU TROVATA. L'allenamento sarÃ  su CPU.\")\n",
    "\n",
    "# 2. Mixed Precision Policy\n",
    "# Nota: PuÃ² causare problemi su alcune architetture/versioni. La disabiliteremo se necessario.\n",
    "policy = tf.keras.mixed_precision.Policy('mixed_float16')\n",
    "tf.keras.mixed_precision.set_global_policy(policy)\n",
    "print(f\"âœ… Politica di Mixed Precision impostata su: {tf.keras.mixed_precision.global_policy().name}\")\n",
    "\n",
    "# 3. Global Constants\n",
    "RANDOM_STATE = 42\n",
    "DATA_PATH = '../data/gtzan/genres_original/'\n",
    "np.random.seed(RANDOM_STATE)\n",
    "tf.random.set_seed(RANDOM_STATE)\n",
    "print(f\"âœ… RiproducibilitÃ  assicurata con RANDOM_STATE = {RANDOM_STATE}\")\n",
    "\n",
    "# 4. Visualization Settings\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette(\"viridis\")\n",
    "print(\"âœ… Ambiente e configurazione pronti.\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1518eb14",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Preprocessing\n",
    "\n",
    "In this section, we define a robust `DataLoader` class to handle the GTZAN dataset. Key features include:\n",
    "- **Dynamic Genre Detection:** Automatically finds genre subfolders.\n",
    "- **Correct Label Encoding:** Fits the `LabelEncoder` immediately to ensure consistency.\n",
    "- **File-Level Splitting:** Splits file paths *before* loading audio to save memory and prevent leakage.\n",
    "- **Spectrogram Extraction:** Converts audio files into Mel spectrograms.\n",
    "- **Shape Unification:** Ensures all spectrograms have a uniform size via padding/truncating.\n",
    "- **Correct Normalization:** Fits the `StandardScaler` **only** on the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b04113d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âŒ ERRORE: Dataset non trovato in ../data/gtzan/genres_original/\n"
     ]
    }
   ],
   "source": [
    "# ===================================================================\n",
    "# CELLA 2: DATA LOADER E PRE-PROCESSING\n",
    "# ===================================================================\n",
    "\n",
    "class GTZANDataLoader:\n",
    "    \"\"\"Carica, pre-processa e suddivide il dataset GTZAN in modo robusto.\"\"\"\n",
    "    def __init__(self, data_path, sample_rate=22050, n_mels=128, hop_length=512):\n",
    "        self.data_path = data_path\n",
    "        self.sample_rate = sample_rate\n",
    "        self.n_mels = n_mels\n",
    "        self.hop_length = hop_length\n",
    "        self.genres = sorted([d for d in os.listdir(data_path) if os.path.isdir(os.path.join(data_path, d))])\n",
    "        self.label_encoder = LabelEncoder().fit(self.genres)\n",
    "        self.scaler = StandardScaler()\n",
    "        print(f\"âœ… GTZANDataLoader inizializzato. Generi: {self.genres}\")\n",
    "        \n",
    "    def load_file_paths(self):\n",
    "        file_paths, labels = [], []\n",
    "        for genre in self.genres:\n",
    "            genre_path = os.path.join(self.data_path, genre)\n",
    "            for filename in os.listdir(genre_path):\n",
    "                if filename.endswith(('.wav', '.au')):\n",
    "                    file_paths.append(os.path.join(genre_path, filename))\n",
    "                    labels.append(genre)\n",
    "        return file_paths, labels\n",
    "    \n",
    "    def process_file(self, file_path, n_segments=5, segment_duration=6):\n",
    "        try:\n",
    "            signal, _ = librosa.load(file_path, sr=self.sample_rate)\n",
    "            samples_per_segment = int(self.sample_rate * segment_duration)\n",
    "            \n",
    "            spectrograms = []\n",
    "            for s in range(n_segments):\n",
    "                start = s * samples_per_segment\n",
    "                end = start + samples_per_segment\n",
    "                if end > len(signal): continue\n",
    "                \n",
    "                mel_spec = librosa.feature.melspectrogram(\n",
    "                    y=signal[start:end], sr=self.sample_rate, n_mels=self.n_mels, hop_length=self.hop_length)\n",
    "                log_mel_spec = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "                spectrograms.append(log_mel_spec)\n",
    "            return spectrograms\n",
    "        except Exception:\n",
    "            return []\n",
    "            \n",
    "    def create_dataset_from_files(self, file_paths, labels_text, n_segments=5):\n",
    "        X_list, y_list = [], []\n",
    "        encoded_labels = self.label_encoder.transform(labels_text)\n",
    "        for i, file_path in enumerate(tqdm(file_paths, desc=f\"Processing {len(file_paths)} files\")):\n",
    "            spectrograms = self.process_file(file_path, n_segments=n_segments)\n",
    "            for spec in spectrograms:\n",
    "                X_list.append(spec)\n",
    "                y_list.append(encoded_labels[i])\n",
    "        return X_list, np.array(y_list)\n",
    "\n",
    "def adjust_spectrograms_shape(spec_list):\n",
    "    target_len = int(np.median([s.shape[1] for s in spec_list if hasattr(s, 'shape')]))\n",
    "    print(f\"   - Uniformazione degli spettrogrammi alla lunghezza: {target_len} frame\")\n",
    "    adjusted_list = []\n",
    "    for spec in spec_list:\n",
    "        if spec.shape[1] > target_len:\n",
    "            adjusted_list.append(spec[:, :target_len])\n",
    "        else:\n",
    "            padding = target_len - spec.shape[1]\n",
    "            adjusted_list.append(np.pad(spec, ((0, 0), (0, padding)), mode='constant'))\n",
    "    return np.array(adjusted_list)\n",
    "\n",
    "# --- ESECUZIONE ---\n",
    "if not os.path.exists(DATA_PATH):\n",
    "    print(f\"âŒ ERRORE: Dataset non trovato in {DATA_PATH}\")\n",
    "else:\n",
    "    data_loader = GTZANDataLoader(data_path=DATA_PATH)\n",
    "    file_paths, labels_text = data_loader.load_file_paths()\n",
    "    \n",
    "    train_files, test_files, train_labels_text, test_labels_text = train_test_split(\n",
    "        file_paths, labels_text, test_size=0.2, random_state=RANDOM_STATE, stratify=labels_text)\n",
    "    train_files, val_files, train_labels_text, val_labels_text = train_test_split(\n",
    "        train_files, train_labels_text, test_size=0.25, random_state=RANDOM_STATE, stratify=train_labels_text)\n",
    "\n",
    "    X_train_list, y_train = data_loader.create_dataset_from_files(train_files, train_labels_text)\n",
    "    X_val_list, y_val = data_loader.create_dataset_from_files(val_files, val_labels_text)\n",
    "    X_test_list, y_test = data_loader.create_dataset_from_files(test_files, test_labels_text)\n",
    "    \n",
    "    X_train = adjust_spectrograms_shape(X_train_list)\n",
    "    X_val = adjust_spectrograms_shape(X_val_list)\n",
    "    X_test = adjust_spectrograms_shape(X_test_list)\n",
    "    \n",
    "    scaler = data_loader.scaler\n",
    "    X_train_shape = X_train.shape\n",
    "    X_train = scaler.fit_transform(X_train.reshape(-1, X_train_shape[1] * X_train_shape[2])).reshape(X_train_shape)\n",
    "    X_val = scaler.transform(X_val.reshape(-1, X_val.shape[1] * X_val.shape[2])).reshape(X_val.shape)\n",
    "    X_test = scaler.transform(X_test.reshape(-1, X_test.shape[1] * X_test.shape[2])).reshape(X_test.shape)\n",
    "    \n",
    "    X_train, X_val, X_test = X_train[..., np.newaxis], X_val[..., np.newaxis], X_test[..., np.newaxis]\n",
    "    \n",
    "    num_classes = len(data_loader.genres)\n",
    "    y_train_cat = to_categorical(y_train, num_classes=num_classes)\n",
    "    y_val_cat = to_categorical(y_val, num_classes=num_classes)\n",
    "    y_test_cat = to_categorical(y_test, num_classes=num_classes)\n",
    "\n",
    "    print(\"\\nðŸ“Š Riepilogo Dati Finali:\")\n",
    "    print(f\"   - Training Set:   X={X_train.shape}, y={y_train_cat.shape}\")\n",
    "    print(f\"   - Validation Set: X={X_val.shape}, y={y_val_cat.shape}\")\n",
    "    print(f\"   - Test Set:       X={X_test.shape}, y={y_test_cat.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c29701",
   "metadata": {},
   "source": [
    "## 3. Model Architectures\n",
    "\n",
    "We define two primary architectures for our analysis:\n",
    "1.  **`UNet_Lite`**: A lightweight version of the U-Net architecture proposed in the paper, designed to be computationally efficient while retaining its multi-scale feature extraction capabilities. It uses strong regularization to combat overfitting.\n",
    "2.  **`SimpleCNN`**: A standard, robust CNN baseline. It serves as a benchmark to determine if the complexity of the U-Net is justified.\n",
    "\n",
    "Both models are implemented as static methods within a `ModelFactory` class for easy instantiation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cca68ef9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ModelFactory definita con i modelli 'UNet_Lite' e 'SimpleCNN'.\n"
     ]
    }
   ],
   "source": [
    "# ===================================================================\n",
    "# CELLA 3: DEFINIZIONE DELLE ARCHITETTURE DEI MODELLI\n",
    "# ===================================================================\n",
    "\n",
    "class ModelFactory:\n",
    "    \"\"\"Contiene i metodi statici per costruire le nostre architetture di modelli.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def _conv_block(inputs, num_filters, l2_reg_factor=0.005):\n",
    "        l2_reg = tf.keras.regularizers.l2(l2_reg_factor)\n",
    "        x = layers.Conv2D(num_filters, (3, 3), padding='same', kernel_regularizer=l2_reg)(inputs)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.PReLU(shared_axes=[1, 2])(x)\n",
    "        x = layers.Conv2D(num_filters, (3, 3), padding='same', kernel_regularizer=l2_reg)(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.PReLU(shared_axes=[1, 2])(x)\n",
    "        return x\n",
    "\n",
    "    @staticmethod\n",
    "    def build_unet_lite_model(input_shape, num_classes):\n",
    "        inputs = layers.Input(shape=input_shape)\n",
    "        x = inputs\n",
    "        h, w = input_shape[0], input_shape[1]\n",
    "        pad_h = (16 - h % 16) % 16\n",
    "        pad_w = (16 - w % 16) % 16\n",
    "        if pad_h > 0 or pad_w > 0:\n",
    "            x = layers.ZeroPadding2D(padding=((0, pad_h), (0, pad_w)))(x)\n",
    "            \n",
    "        conv1 = ModelFactory._conv_block(x, 32)\n",
    "        pool1 = layers.MaxPooling2D((2, 2))(conv1)\n",
    "        conv2 = ModelFactory._conv_block(pool1, 64)\n",
    "        pool2 = layers.MaxPooling2D((2, 2))(conv2)\n",
    "        conv3 = ModelFactory._conv_block(pool2, 128)\n",
    "        pool3 = layers.MaxPooling2D((2, 2))(conv3)\n",
    "        conv4 = ModelFactory._conv_block(pool3, 256)\n",
    "        pool4 = layers.MaxPooling2D((2, 2))(conv4)\n",
    "        \n",
    "        bottleneck = ModelFactory._conv_block(pool4, 512)\n",
    "        \n",
    "        up6 = layers.Conv2DTranspose(256, (2, 2), strides=(2, 2), padding='same')(bottleneck)\n",
    "        merge6 = layers.concatenate([conv4, up6])\n",
    "        conv6 = ModelFactory._conv_block(merge6, 256)\n",
    "        \n",
    "        up7 = layers.Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(conv6)\n",
    "        merge7 = layers.concatenate([conv3, up7])\n",
    "        conv7 = ModelFactory._conv_block(merge7, 128)\n",
    "        \n",
    "        up8 = layers.Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(conv7)\n",
    "        merge8 = layers.concatenate([conv2, up8])\n",
    "        conv8 = ModelFactory._conv_block(merge8, 64)\n",
    "        \n",
    "        up9 = layers.Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same')(conv8)\n",
    "        merge9 = layers.concatenate([conv1, up9])\n",
    "        conv9 = ModelFactory._conv_block(merge9, 32)\n",
    "\n",
    "        if pad_h > 0 or pad_w > 0:\n",
    "            final_conv = layers.Cropping2D(cropping=((0, pad_h), (0, pad_w)))(conv9)\n",
    "        else:\n",
    "            final_conv = conv9\n",
    "\n",
    "        gap = layers.GlobalAveragePooling2D()(final_conv)\n",
    "        x = layers.Dense(256, activation='relu')(gap)\n",
    "        x = layers.Dropout(0.6)(x)\n",
    "        x = layers.Dense(128, activation='relu')(x)\n",
    "        x = layers.Dropout(0.6)(x)\n",
    "        outputs = layers.Dense(num_classes, activation='softmax', dtype='float32')(x)\n",
    "        \n",
    "        return models.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "    @staticmethod\n",
    "    def build_simple_cnn(input_shape, num_classes):\n",
    "        inputs = layers.Input(shape=input_shape)\n",
    "        x = inputs\n",
    "        \n",
    "        x = layers.Conv2D(32, (3, 3), padding='same')(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Activation('relu')(x)\n",
    "        x = layers.MaxPooling2D((2, 2))(x)\n",
    "        \n",
    "        x = layers.Conv2D(64, (3, 3), padding='same')(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Activation('relu')(x)\n",
    "        x = layers.MaxPooling2D((2, 2))(x)\n",
    "        \n",
    "        x = layers.Conv2D(128, (3, 3), padding='same')(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Activation('relu')(x)\n",
    "        \n",
    "        x = layers.GlobalAveragePooling2D()(x)\n",
    "        x = layers.Dense(128, activation='relu')(x)\n",
    "        x = layers.Dropout(0.5)(x)\n",
    "        outputs = layers.Dense(num_classes, activation='softmax', dtype='float32')(x)\n",
    "        \n",
    "        return models.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "print(\"âœ… ModelFactory definita con i modelli 'UNet_Lite' e 'SimpleCNN'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe004fc",
   "metadata": {},
   "source": [
    "## 4. Training and Evaluation\n",
    "\n",
    "This section defines the framework for training and evaluating our models. We use `tf.data` for efficient data pipelines and a custom `Evaluator` class to orchestrate the experiments.\n",
    "\n",
    "### Data Augmentation\n",
    "- **SpecAugment** is applied on-the-fly to the training data. This technique randomly masks frequency bands and time steps in the spectrograms, forcing the model to learn more robust features.\n",
    "\n",
    "### Training Loop\n",
    "- Each model (`UNet_Lite`, `SimpleCNN`) is trained with each optimizer (`Adam`, `SGD_Momentum`, `RMSprop`).\n",
    "- We use `EarlyStopping` to prevent overfitting and `ReduceLROnPlateau` to adjust the learning rate.\n",
    "- All results, models, and training histories are stored for later analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3e3390",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 132\u001b[39m\n\u001b[32m    129\u001b[39m BATCH_SIZE = \u001b[32m64\u001b[39m\n\u001b[32m    130\u001b[39m EPOCHS = \u001b[32m60\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m132\u001b[39m train_pipeline = tf.data.Dataset.from_tensor_slices((\u001b[43mX_train\u001b[49m, y_train_cat)).shuffle(\u001b[38;5;28mlen\u001b[39m(X_train)).map(spec_augment, AUTOTUNE).batch(BATCH_SIZE).prefetch(AUTOTUNE)\n\u001b[32m    133\u001b[39m val_pipeline = tf.data.Dataset.from_tensor_slices((X_val, y_val_cat)).batch(BATCH_SIZE).prefetch(AUTOTUNE)\n\u001b[32m    134\u001b[39m test_pipeline = tf.data.Dataset.from_tensor_slices((X_test, y_test_cat)).batch(BATCH_SIZE).prefetch(AUTOTUNE)\n",
      "\u001b[31mNameError\u001b[39m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "# ===================================================================\n",
    "# CELLA 4: FRAMEWORK DI TRAINING E ESECUZIONE ESPERIMENTI\n",
    "# ===================================================================\n",
    "\n",
    "class MusicGenreEvaluator:\n",
    "    \"\"\"Orchestra il training, la valutazione e l'analisi dei modelli.\"\"\"\n",
    "    def __init__(self, class_names):\n",
    "        self.class_names = class_names\n",
    "        self.results = {}\n",
    "        self.histories = {}\n",
    "        self.test_evals = {}\n",
    "\n",
    "    def prepare_optimizers(self, lr_adam=1e-3, lr_sgd=1e-2, lr_rms=1e-3):\n",
    "        return {\n",
    "            'Adam': optimizers.Adam(learning_rate=lr_adam),\n",
    "            'SGD_Momentum': optimizers.SGD(learning_rate=lr_sgd, momentum=0.9),\n",
    "            'RMSprop': optimizers.RMSprop(learning_rate=lr_rms)\n",
    "        }\n",
    "\n",
    "    def run_experiments(self, model_factories, optimizers_config, train_data, val_data, test_data, epochs):\n",
    "        for model_name, model_factory in model_factories.items():\n",
    "            print(f\"\\n{'='*80}\\nARCHITETTURA IN TEST: '{model_name}'\\n{'='*80}\")\n",
    "            temp_model = model_factory()\n",
    "            temp_model.summary(line_length=110)\n",
    "            del temp_model\n",
    "\n",
    "            for optimizer_name, optimizer in optimizers_config.items():\n",
    "                print(f\"\\n--- ðŸš€ TRAINING: [{model_name}] with [{optimizer_name}] ---\")\n",
    "                \n",
    "                # Risoluzione del problema SimpleCNN + Mixed Precision\n",
    "                # Disabilitiamo temporaneamente la mixed precision per la SimpleCNN\n",
    "                is_simple_cnn = 'SimpleCNN' in model_name\n",
    "                if is_simple_cnn:\n",
    "                    tf.keras.mixed_precision.set_global_policy('float32')\n",
    "                    print(\"   - Avviso: Mixed Precision disabilitata per SimpleCNN per stabilitÃ .\")\n",
    "                \n",
    "                model = model_factory()\n",
    "                model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "                \n",
    "                callbacks_list = [\n",
    "                    callbacks.EarlyStopping(monitor='val_accuracy', patience=15, restore_best_weights=True, verbose=1),\n",
    "                    callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, verbose=1)\n",
    "                ]\n",
    "                \n",
    "                history = model.fit(\n",
    "                    train_data, epochs=epochs, validation_data=val_data,\n",
    "                    callbacks=callbacks_list, verbose=2\n",
    "                )\n",
    "                \n",
    "                experiment_key = f\"{model_name}_{optimizer_name}\"\n",
    "                self.histories[experiment_key] = history\n",
    "                \n",
    "                print(f\"\\n--- ðŸ§ª VALUTAZIONE su Test Set: [{experiment_key}] ---\")\n",
    "                test_loss, test_acc = model.evaluate(test_data, verbose=0)\n",
    "                self.results[experiment_key] = {'test_accuracy': test_acc, 'test_loss': test_loss}\n",
    "                print(f\"   - Test Loss: {test_loss:.4f} | Test Accuracy: {test_acc:.4f}\")\n",
    "                \n",
    "                y_pred = model.predict(test_data)\n",
    "                y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "                y_true = np.concatenate([y for x, y in test_data], axis=0)\n",
    "                y_true_classes = np.argmax(y_true, axis=1)\n",
    "                self.test_evals[experiment_key] = {\n",
    "                    'report': classification_report(y_true_classes, y_pred_classes, target_names=self.class_names),\n",
    "                    'cm': confusion_matrix(y_true_classes, y_pred_classes)\n",
    "                }\n",
    "\n",
    "                # Reimposta la policy a mixed_float16 per il prossimo modello\n",
    "                if is_simple_cnn:\n",
    "                    tf.keras.mixed_precision.set_global_policy('mixed_float16')\n",
    "                    print(\"   - Avviso: Mixed Precision riabilitata.\")\n",
    "\n",
    "        return self.results, self.histories, self.test_evals\n",
    "    \n",
    "    def plot_results(self):\n",
    "        # ... (Funzioni di plotting da implementare qui) ...\n",
    "        print(\"\\n--- Visualizzazione Risultati ---\")\n",
    "        # Esempio di plot\n",
    "        results_df = pd.DataFrame(self.results).T.reset_index().rename(columns={'index': 'Experiment'})\n",
    "        results_df[['Model', 'Optimizer']] = results_df['Experiment'].str.split('_', n=1, expand=True)\n",
    "        \n",
    "        plt.figure(figsize=(14, 6))\n",
    "        sns.barplot(data=results_df, x='Experiment', y='test_accuracy', hue='Model')\n",
    "        plt.title('Confronto Accuratezza su Test Set', fontsize=16)\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.xlabel('Esperimento (Modello + Ottimizzatore)')\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.ylim(0, 1)\n",
    "        plt.grid(axis='y')\n",
    "        plt.show()\n",
    "\n",
    "# --- Creazione Pipeline Dati e Esecuzione ---\n",
    "\n",
    "def spec_augment(spectrogram, label):\n",
    "    spectrogram_augmented = tf.identity(spectrogram)\n",
    "    if spectrogram.ndim == 3 and spectrogram.shape[-1] == 1:\n",
    "        spectrogram_augmented = tf.squeeze(spectrogram_augmented, axis=-1)\n",
    "    \n",
    "    freq_bins, time_steps = tf.shape(spectrogram_augmented)[0], tf.shape(spectrogram_augmented)[1]\n",
    "    \n",
    "    f_param = tf.cast(tf.cast(freq_bins, tf.float32) * 0.2, tf.int32)\n",
    "    f = tf.random.uniform(shape=(), minval=0, maxval=f_param, dtype=tf.int32)\n",
    "    f0 = tf.random.uniform(shape=(), minval=0, maxval=freq_bins - f)\n",
    "    \n",
    "    mask_start = tf.cast(f0, dtype=tf.int32)\n",
    "    mask_end = tf.cast(f0 + f, dtype=tf.int32)\n",
    "    \n",
    "    indices = tf.range(mask_start, mask_end)\n",
    "    spectrogram_augmented = tf.tensor_scatter_nd_update(\n",
    "        spectrogram_augmented, tf.expand_dims(indices, axis=1), \n",
    "        tf.zeros((f, time_steps), dtype=spectrogram.dtype))\n",
    "\n",
    "    t_param = tf.cast(tf.cast(time_steps, tf.float32) * 0.2, tf.int32)\n",
    "    t = tf.random.uniform(shape=(), minval=0, maxval=t_param, dtype=tf.int32)\n",
    "    t0 = tf.random.uniform(shape=(), minval=0, maxval=time_steps - t)\n",
    "    \n",
    "    mask_start = tf.cast(t0, dtype=tf.int32)\n",
    "    mask_end = tf.cast(t0 + t, dtype=tf.int32)\n",
    "    \n",
    "    indices = tf.range(mask_start, mask_end)\n",
    "    spectrogram_augmented = tf.transpose(spectrogram_augmented)\n",
    "    spectrogram_augmented = tf.tensor_scatter_nd_update(\n",
    "        spectrogram_augmented, tf.expand_dims(indices, axis=1), \n",
    "        tf.zeros((t, freq_bins), dtype=spectrogram.dtype))\n",
    "    spectrogram_augmented = tf.transpose(spectrogram_augmented)\n",
    "\n",
    "    return tf.expand_dims(spectrogram_augmented, axis=-1), label\n",
    "\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 60\n",
    "\n",
    "train_pipeline = tf.data.Dataset.from_tensor_slices((X_train, y_train_cat)).shuffle(len(X_train)).map(spec_augment, AUTOTUNE).batch(BATCH_SIZE).prefetch(AUTOTUNE)\n",
    "val_pipeline = tf.data.Dataset.from_tensor_slices((X_val, y_val_cat)).batch(BATCH_SIZE).prefetch(AUTOTUNE)\n",
    "test_pipeline = tf.data.Dataset.from_tensor_slices((X_test, y_test_cat)).batch(BATCH_SIZE).prefetch(AUTOTUNE)\n",
    "\n",
    "input_shape = X_train.shape[1:]\n",
    "num_classes = y_train_cat.shape[1]\n",
    "\n",
    "model_factories = {\n",
    "    'UNet_Lite': lambda: ModelFactory.build_unet_lite_model(input_shape, num_classes),\n",
    "    'SimpleCNN': lambda: ModelFactory.build_simple_cnn(input_shape, num_classes),\n",
    "}\n",
    "\n",
    "evaluator = MusicGenreEvaluator(class_names=data_loader.genres)\n",
    "optimizers_config = evaluator.prepare_optimizers()\n",
    "results, histories, test_evals = evaluator.run_experiments(model_factories, optimizers_config, train_pipeline, val_pipeline, test_pipeline, EPOCHS)\n",
    "\n",
    "evaluator.plot_results()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
