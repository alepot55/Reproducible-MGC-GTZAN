{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab25c766",
   "metadata": {},
   "source": [
    "# 01b — Model Training on FMA (UNet_Audio_Classifier)\n",
    "\n",
    "Allena solo `UNet_Audio_Classifier` su FMA e salva risultati.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4bd82ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logger (consistent with setup notebook)\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "VERBOSE = os.environ.get('FMA_VERBOSE', '1') == '1'\n",
    "\n",
    "def log(msg: str, level: str = 'INFO'):\n",
    "    if not VERBOSE and level == 'INFO':\n",
    "        return\n",
    "    ts = datetime.now().strftime('%H:%M:%S')\n",
    "    print(f'[{ts}] {level}: {msg}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b47187a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[09:02:51] INFO: Config -> FORCE_RETRAIN=False, RESUME_FROM_CKPT=False, SKIP_TRAIN_IF_CKPT=False, EPOCHS=80, PATIENCE=15, BATCH=32, SPEC_AUG=True, LS=0.05\n"
     ]
    }
   ],
   "source": [
    "# Training config toggles (override via env if needed)\n",
    "\n",
    "\n",
    "# Set to '1' to ignore checkpoint and train from scratch\n",
    "FMA_FORCE_RETRAIN = os.environ.get('FMA_FORCE_RETRAIN', '0') == '1'\n",
    "\n",
    "\n",
    "# Set to '1' to RESUME training from existing checkpoint (continue training)\n",
    "FMA_RESUME_FROM_CKPT = os.environ.get('FMA_RESUME_FROM_CKPT', '0') == '1'\n",
    "\n",
    "\n",
    "# If '1' and checkpoint exists (and not forcing/resuming), skip training\n",
    "SKIP_TRAIN_IF_CKPT = os.environ.get('SKIP_TRAIN_IF_CKPT', '0') == '1'\n",
    "\n",
    "\n",
    "# Training hyperparams\n",
    "FMA_EPOCHS = int(os.environ.get('FMA_EPOCHS', '80'))\n",
    "FMA_PATIENCE = int(os.environ.get('FMA_PATIENCE', '15'))\n",
    "\n",
    "\n",
    "# Batch size (reuse your env var if set)\n",
    "FMA_BATCH_SIZE = int(os.environ.get('INFER_BATCH_SIZE', os.environ.get('FMA_BATCH_SIZE', '32')))\n",
    "\n",
    "\n",
    "# Regularization / Augmentation\n",
    "FMA_LABEL_SMOOTH = float(os.environ.get('FMA_LABEL_SMOOTH', '0.05'))  # small smoothing helps generalization\n",
    "FMA_SPEC_AUGMENT = os.environ.get('FMA_SPEC_AUGMENT', '1') == '1'\n",
    "FMA_FREQ_MASK_PARAM = int(os.environ.get('FMA_FREQ_MASK_PARAM', '16'))\n",
    "FMA_TIME_MASK_PARAM = int(os.environ.get('FMA_TIME_MASK_PARAM', '32'))\n",
    "FMA_NUM_MASKS = int(os.environ.get('FMA_NUM_MASKS', '2'))\n",
    "\n",
    "\n",
    "log(f\"Config -> FORCE_RETRAIN={FMA_FORCE_RETRAIN}, RESUME_FROM_CKPT={FMA_RESUME_FROM_CKPT}, SKIP_TRAIN_IF_CKPT={SKIP_TRAIN_IF_CKPT}, EPOCHS={FMA_EPOCHS}, PATIENCE={FMA_PATIENCE}, BATCH={FMA_BATCH_SIZE}, SPEC_AUG={FMA_SPEC_AUGMENT}, LS={FMA_LABEL_SMOOTH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ccec9d5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-25 09:02:51.816121: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-08-25 09:02:51.830827: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1756105371.846798   49089 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1756105371.851514   49089 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1756105371.864049   49089 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1756105371.864071   49089 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1756105371.864073   49089 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1756105371.864075   49089 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-08-25 09:02:51.868955: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[09:02:54] INFO: FMA shapes (norm): train=(1920, 128, 129, 1), val=(640, 128, 129, 1), test=(640, 128, 129, 1) | classes: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-25 09:02:54.333326: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2025-08-25 09:02:54.333389: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:167] env: CUDA_VISIBLE_DEVICES=\"-1\"\n",
      "2025-08-25 09:02:54.333400: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:170] CUDA_VISIBLE_DEVICES is set to -1 - this hides all GPUs from CUDA\n",
      "2025-08-25 09:02:54.333407: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:178] verbose logging is disabled. Rerun with verbose logging (usually --v=1 or --vmodule=cuda_diagnostics=1) to get more diagnostic output from this module\n",
      "2025-08-25 09:02:54.333415: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:183] retrieving CUDA diagnostic information for host: Jhonny\n",
      "2025-08-25 09:02:54.333418: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:190] hostname: Jhonny\n",
      "2025-08-25 09:02:54.333635: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:197] libcuda reported version is: 575.64.3\n",
      "2025-08-25 09:02:54.333656: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:201] kernel reported version is: 575.64.3\n",
      "2025-08-25 09:02:54.333658: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:291] kernel version seems to match DSO: 575.64.3\n"
     ]
    }
   ],
   "source": [
    "# Load processed FMA data (clean setup)\n",
    "import os\n",
    "# Toggle: set TRAIN_ON_GPU=1 in env to enable GPU; default CPU to avoid CUDA conflicts in notebooks\n",
    "if os.environ.get('TRAIN_ON_GPU', '0') != '1':\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "\n",
    "import pickle, time, numpy as np, pandas as pd, tensorflow as tf\n",
    "import keras\n",
    "from keras import layers, models, callbacks\n",
    "from keras.utils import to_categorical\n",
    "from pathlib import Path\n",
    "\n",
    "# Reproducibility and safer TF setup\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "tf.random.set_seed(RANDOM_STATE)\n",
    "\n",
    "# Enable GPU memory growth if GPUs are present to prevent OOM crashes (no-op when CPU-only)\n",
    "try:\n",
    "    gpus = tf.config.list_physical_devices('GPU')\n",
    "    for gpu in gpus:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "except Exception as e:\n",
    "    log(f'TF GPU setup warning: {e}', level='WARN')\n",
    "\n",
    "PROJECT_ROOT = Path(os.getcwd()).resolve().parents[1]\n",
    "PROCESSED = PROJECT_ROOT/'data'/'processed_fma'\n",
    "MODELS = PROJECT_ROOT/'models'\n",
    "REPORTS = PROJECT_ROOT/'reports'\n",
    "MODELS.mkdir(exist_ok=True); REPORTS.mkdir(exist_ok=True)\n",
    "\n",
    "# Load arrays (memory-mapped to reduce RAM spikes)\n",
    "X_train = np.load(PROCESSED/'X_train.npy', mmap_mode='r'); y_train = np.load(PROCESSED/'y_train.npy', mmap_mode='r')\n",
    "X_val = np.load(PROCESSED/'X_val.npy', mmap_mode='r'); y_val = np.load(PROCESSED/'y_val.npy', mmap_mode='r')\n",
    "X_test = np.load(PROCESSED/'X_test.npy', mmap_mode='r'); y_test = np.load(PROCESSED/'y_test.npy', mmap_mode='r')\n",
    "\n",
    "# Ensure channel dimension exists and time dimension matches across splits\n",
    "# Shapes expected: (N, n_mels=128, n_frames, 1)\n",
    "if X_train.ndim == 3:\n",
    "    X_train = X_train[..., None]\n",
    "if X_val.ndim == 3:\n",
    "    X_val = X_val[..., None]\n",
    "if X_test.ndim == 3:\n",
    "    X_test = X_test[..., None]\n",
    "\n",
    "# Align time dimension by cropping/padding to the minimum frames across splits\n",
    "T_train, T_val, T_test = X_train.shape[2], X_val.shape[2], X_test.shape[2]\n",
    "T_min = int(min(T_train, T_val, T_test))\n",
    "if not (T_train == T_val == T_test):\n",
    "    log(f'Time frames mismatch detected (train={T_train}, val={T_val}, test={T_test}). Normalizing to T={T_min}.', level='WARN')\n",
    "\n",
    "def _pad_or_crop_time(X, T):\n",
    "    cur = X.shape[2]\n",
    "    if cur == T:\n",
    "        return X\n",
    "    if cur > T:\n",
    "        return X[:, :, :T, :]\n",
    "    # pad at end with zeros\n",
    "    pad_width = ((0,0),(0,0),(0, T - cur),(0,0))\n",
    "    return np.pad(np.asarray(X), pad_width, mode='constant')\n",
    "\n",
    "X_train = _pad_or_crop_time(X_train, T_min)\n",
    "X_val = _pad_or_crop_time(X_val, T_min)\n",
    "X_test = _pad_or_crop_time(X_test, T_min)\n",
    "\n",
    "with open(PROCESSED/'label_encoder.pkl','rb') as f: le = pickle.load(f)\n",
    "num_classes = len(le.classes_)\n",
    "y_train_cat = to_categorical(y_train, num_classes)\n",
    "y_val_cat = to_categorical(y_val, num_classes)\n",
    "y_test_cat = to_categorical(y_test, num_classes)\n",
    "\n",
    "log(f'FMA shapes (norm): train={X_train.shape}, val={X_val.shape}, test={X_test.shape} | classes: {num_classes}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ada5244e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[09:02:54] INFO: Num classes: 8\n",
      "[09:02:54] INFO: Class names: [np.str_('Electronic'), np.str_('Experimental'), np.str_('Folk'), np.str_('Hip-Hop'), np.str_('Instrumental'), np.str_('International'), np.str_('Pop'), np.str_('Rock')]\n",
      "[09:02:54] INFO: train distribution: [240, 240, 240, 240, 240, 240, 240, 240] | total: 1920\n",
      "[09:02:54] INFO: val distribution: [80, 80, 80, 80, 80, 80, 80, 80] | total: 640\n",
      "[09:02:54] INFO: test distribution: [80, 80, 80, 80, 80, 80, 80, 80] | total: 640\n"
     ]
    }
   ],
   "source": [
    "# Sanity checks: class distributions and label names\n",
    "\n",
    "log(f'Num classes: {num_classes}')\n",
    "# Use list() to support both numpy arrays and Python lists without relying on .tolist()\n",
    "class_names = list(getattr(le, \"classes_\", []))\n",
    "log(f'Class names: {class_names}')\n",
    "\n",
    "for split_name, y in [('train', y_train_cat), ('val', y_val_cat), ('test', y_test_cat)]:\n",
    "    counts = np.sum(y, axis=0).astype(int)\n",
    "    log(f'{split_name} distribution: {counts.tolist()} | total: {int(np.sum(counts))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "775cbeda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional SpecAugment for training only (frequency/time masking)\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def spec_augment(mel, freq_mask_param=16, time_mask_param=32, num_masks=2):\n",
    "    \"\"\"Apply SpecAugment masks to a single mel-spectrogram tensor.\n",
    "    mel: float32 tensor [M, T, 1]\n",
    "    Returns augmented tensor with same shape.\n",
    "    \"\"\"\n",
    "    M = tf.shape(mel)[0]\n",
    "    T = tf.shape(mel)[1]\n",
    "    x = mel\n",
    "    for _ in range(num_masks):\n",
    "        if FMA_FREQ_MASK_PARAM > 0:\n",
    "            f = tf.random.uniform([], minval=0, maxval=freq_mask_param + 1, dtype=tf.int32)\n",
    "            f0 = tf.random.uniform([], minval=0, maxval=tf.maximum(M - f, 1), dtype=tf.int32)\n",
    "            mask = tf.concat([tf.ones([f0, T, 1]), tf.zeros([f, T, 1]), tf.ones([M - f - f0, T, 1])], axis=0)\n",
    "            x = x * mask\n",
    "        if FMA_TIME_MASK_PARAM > 0:\n",
    "            t = tf.random.uniform([], minval=0, maxval=time_mask_param + 1, dtype=tf.int32)\n",
    "            t0 = tf.random.uniform([], minval=0, maxval=tf.maximum(T - t, 1), dtype=tf.int32)\n",
    "            mask = tf.concat([tf.ones([M, t0, 1]), tf.zeros([M, t, 1]), tf.ones([M, T - t - t0, 1])], axis=1)\n",
    "            x = x * mask\n",
    "    return x\n",
    "\n",
    "\n",
    "def ds_with_optional_aug(X, y_cat, batch_size, training: bool):\n",
    "    ds = tf.data.Dataset.from_tensor_slices((X, y_cat))\n",
    "    if training:\n",
    "        ds = ds.shuffle(min(10000, len(X)))\n",
    "    ds = ds.batch(batch_size)\n",
    "    if training and FMA_SPEC_AUGMENT:\n",
    "        def _aug(mel, y):\n",
    "            mel = tf.map_fn(lambda m: spec_augment(m, FMA_FREQ_MASK_PARAM, FMA_TIME_MASK_PARAM, FMA_NUM_MASKS), mel, fn_output_signature=mel.dtype)\n",
    "            return mel, y\n",
    "        ds = ds.map(_aug, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    ds = ds.prefetch(tf.data.AUTOTUNE)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f53cdb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[09:02:54] INFO: Train counts: {np.str_('Electronic'): 240, np.str_('Experimental'): 240, np.str_('Folk'): 240, np.str_('Hip-Hop'): 240, np.str_('Instrumental'): 240, np.str_('International'): 240, np.str_('Pop'): 240, np.str_('Rock'): 240}\n",
      "[09:02:54] INFO: Val counts: {np.str_('Electronic'): 80, np.str_('Experimental'): 80, np.str_('Folk'): 80, np.str_('Hip-Hop'): 80, np.str_('Instrumental'): 80, np.str_('International'): 80, np.str_('Pop'): 80, np.str_('Rock'): 80}\n",
      "[09:02:54] INFO: Test counts: {np.str_('Electronic'): 80, np.str_('Experimental'): 80, np.str_('Folk'): 80, np.str_('Hip-Hop'): 80, np.str_('Instrumental'): 80, np.str_('International'): 80, np.str_('Pop'): 80, np.str_('Rock'): 80}\n"
     ]
    }
   ],
   "source": [
    "# Assert multi-class splits and show per-class counts (indices -> names)\n",
    "from collections import Counter\n",
    "\n",
    "# Use integer labels directly for robust counting\n",
    "train_counts = Counter(np.asarray(y_train).tolist())\n",
    "val_counts = Counter(np.asarray(y_val).tolist())\n",
    "test_counts = Counter(np.asarray(y_test).tolist())\n",
    "\n",
    "idx_to_name = {i: cls for i, cls in enumerate(le.classes_)}\n",
    "\n",
    "def fmt_counts(cntr):\n",
    "    return {idx_to_name.get(i, str(i)): int(n) for i, n in sorted(cntr.items())}\n",
    "\n",
    "log(f'Train counts: {fmt_counts(train_counts)}')\n",
    "log(f'Val counts: {fmt_counts(val_counts)}')\n",
    "log(f'Test counts: {fmt_counts(test_counts)}')\n",
    "\n",
    "if len(train_counts) < 2:\n",
    "    raise RuntimeError(\n",
    "        'Data preparation appears to be single-class on train. Please re-run the FMA Setup notebook to regenerate processed arrays with multiple classes.'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6cafd8f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"UNet_Audio_Classifier\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"UNet_Audio_Classifier\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">288</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ p_re_lu (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">PReLU</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │            <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │         <span style=\"color: #00af00; text-decoration-color: #00af00\">9,216</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ p_re_lu_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">PReLU</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │            <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,432</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_2           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ p_re_lu_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">PReLU</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │            <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">73,728</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_3           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ p_re_lu_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">PReLU</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)              │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,032</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m129\u001b[0m, \u001b[38;5;34m1\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m129\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │           \u001b[38;5;34m288\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m129\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │           \u001b[38;5;34m128\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ p_re_lu (\u001b[38;5;33mPReLU\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m129\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │            \u001b[38;5;34m32\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m129\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │         \u001b[38;5;34m9,216\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m129\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │           \u001b[38;5;34m128\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ p_re_lu_1 (\u001b[38;5;33mPReLU\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m129\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │            \u001b[38;5;34m32\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d (\u001b[38;5;33mMaxPooling2D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │        \u001b[38;5;34m18,432\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_2           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │           \u001b[38;5;34m256\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ p_re_lu_2 (\u001b[38;5;33mPReLU\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │            \u001b[38;5;34m64\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_1 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_3 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │        \u001b[38;5;34m73,728\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_3           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │           \u001b[38;5;34m512\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ p_re_lu_3 (\u001b[38;5;33mPReLU\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │           \u001b[38;5;34m128\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m)              │         \u001b[38;5;34m1,032\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">103,976</span> (406.16 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m103,976\u001b[0m (406.16 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">103,464</span> (404.16 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m103,464\u001b[0m (404.16 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> (2.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m512\u001b[0m (2.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define UNet_Audio_Classifier (lean variant matching project baseline)\n",
    "from keras import layers, models\n",
    "\n",
    "def build_unet_audio_classifier(input_shape, num_classes):\n",
    "    i = layers.Input(shape=input_shape)\n",
    "    x = layers.Conv2D(32,3,padding='same',use_bias=False)(i); x = layers.BatchNormalization()(x); x = layers.PReLU(shared_axes=[1,2])(x)\n",
    "    x = layers.Conv2D(32,3,padding='same',use_bias=False)(x); x = layers.BatchNormalization()(x); x = layers.PReLU(shared_axes=[1,2])(x)\n",
    "    x = layers.MaxPooling2D(2)(x)\n",
    "    x = layers.Conv2D(64,3,padding='same',use_bias=False)(x); x = layers.BatchNormalization()(x); x = layers.PReLU(shared_axes=[1,2])(x)\n",
    "    x = layers.MaxPooling2D(2)(x)\n",
    "    x = layers.Conv2D(128,3,padding='same',use_bias=False)(x); x = layers.BatchNormalization()(x); x = layers.PReLU(shared_axes=[1,2])(x)\n",
    "    x = layers.GlobalAveragePooling2D()(x); x = layers.Dropout(0.5)(x)\n",
    "    o = layers.Dense(num_classes, activation='softmax', dtype='float32')(x)\n",
    "    return models.Model(i,o,name='UNet_Audio_Classifier')\n",
    "\n",
    "input_shape = tuple(int(d) for d in X_train.shape[1:])\n",
    "model = build_unet_audio_classifier(input_shape, num_classes)\n",
    "model.compile(optimizer=keras.optimizers.Adam(1e-3), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e8864b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[09:02:55] INFO: Class weights: {0: 1.0, 1: 1.0, 2: 1.0, 3: 1.0, 4: 1.0, 5: 1.0, 6: 1.0, 7: 1.0}\n",
      "[09:02:55] INFO: No checkpoint scenario → training from scratch\n",
      "Epoch 1/80\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 611ms/step - accuracy: 0.2401 - loss: 2.0574 - val_accuracy: 0.1797 - val_loss: 2.1565 - learning_rate: 0.0010\n",
      "Epoch 2/80\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 583ms/step - accuracy: 0.2829 - loss: 1.9330 - val_accuracy: 0.1437 - val_loss: 2.1988 - learning_rate: 0.0010\n",
      "Epoch 3/80\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 589ms/step - accuracy: 0.3089 - loss: 1.9122 - val_accuracy: 0.1703 - val_loss: 2.2894 - learning_rate: 0.0010\n",
      "Epoch 4/80\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 581ms/step - accuracy: 0.3680 - loss: 1.7648 - val_accuracy: 0.1547 - val_loss: 2.3295 - learning_rate: 0.0010\n",
      "Epoch 5/80\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 600ms/step - accuracy: 0.3830 - loss: 1.7406 - val_accuracy: 0.2141 - val_loss: 2.3082 - learning_rate: 0.0010\n",
      "Epoch 6/80\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 607ms/step - accuracy: 0.4052 - loss: 1.6829 - val_accuracy: 0.1922 - val_loss: 2.9232 - learning_rate: 0.0010\n",
      "Epoch 7/80\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 594ms/step - accuracy: 0.4020 - loss: 1.6623 - val_accuracy: 0.2297 - val_loss: 2.5599 - learning_rate: 0.0010\n",
      "Epoch 8/80\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 601ms/step - accuracy: 0.4100 - loss: 1.6604 - val_accuracy: 0.3281 - val_loss: 2.1640 - learning_rate: 0.0010\n",
      "Epoch 9/80\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 590ms/step - accuracy: 0.4656 - loss: 1.6004 - val_accuracy: 0.3875 - val_loss: 1.9645 - learning_rate: 2.0000e-04\n",
      "Epoch 10/80\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 586ms/step - accuracy: 0.4442 - loss: 1.5712 - val_accuracy: 0.3578 - val_loss: 2.0681 - learning_rate: 2.0000e-04\n",
      "Epoch 11/80\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 577ms/step - accuracy: 0.4814 - loss: 1.5289 - val_accuracy: 0.3703 - val_loss: 2.0962 - learning_rate: 2.0000e-04\n",
      "Epoch 12/80\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 602ms/step - accuracy: 0.4670 - loss: 1.5338 - val_accuracy: 0.4016 - val_loss: 1.9195 - learning_rate: 2.0000e-04\n",
      "Epoch 13/80\n",
      "\u001b[1m16/60\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m23s\u001b[0m 541ms/step - accuracy: 0.5282 - loss: 1.4428"
     ]
    }
   ],
   "source": [
    "# Train with clean logs and checkpoint skip\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "# Use smaller, safer batch size if memory is tight; can be increased later\n",
    "BATCH_SIZE = FMA_BATCH_SIZE\n",
    "\n",
    "ckpt_path = MODELS/'UNet_Audio_Classifier_best_FMA.keras'\n",
    "\n",
    "# Helper to crop/pad along mel/time dims to match target shape\n",
    "def _pad_or_crop_axis(X, axis: int, target: int):\n",
    "    cur = X.shape[axis]\n",
    "    if cur == target:\n",
    "        return X\n",
    "    if cur > target:\n",
    "        # crop\n",
    "        slicer = [slice(None)] * X.ndim\n",
    "        slicer[axis] = slice(0, target)\n",
    "        return X[tuple(slicer)]\n",
    "    # pad zeros at end\n",
    "    pad_width = [(0,0)] * X.ndim\n",
    "    pad_width[axis] = (0, target - cur)\n",
    "    return np.pad(np.asarray(X), pad_width, mode='constant')\n",
    "\n",
    "# Build model or load/resume from checkpoint per toggles\n",
    "model = None\n",
    "loaded_ckpt = False\n",
    "if ckpt_path.exists() and not FMA_FORCE_RETRAIN:\n",
    "    if FMA_RESUME_FROM_CKPT:\n",
    "        try:\n",
    "            model = keras.models.load_model(ckpt_path, compile=False)\n",
    "            model.compile(optimizer=keras.optimizers.Adam(1e-3), loss=keras.losses.CategoricalCrossentropy(label_smoothing=FMA_LABEL_SMOOTH), metrics=['accuracy'])\n",
    "            loaded_ckpt = True\n",
    "            log(f'Resuming training from checkpoint: {ckpt_path}')\n",
    "        except Exception as e:\n",
    "            log(f'Checkpoint load failed ({e}); building a new model', level='WARN')\n",
    "    elif SKIP_TRAIN_IF_CKPT:\n",
    "        try:\n",
    "            model = keras.models.load_model(ckpt_path, compile=False)\n",
    "            model.compile(optimizer=keras.optimizers.Adam(1e-3), loss=keras.losses.CategoricalCrossentropy(label_smoothing=FMA_LABEL_SMOOTH), metrics=['accuracy'])\n",
    "            loaded_ckpt = True\n",
    "            log(f'Loaded existing checkpoint: {ckpt_path} — will skip training')\n",
    "        except Exception as e:\n",
    "            log(f'Checkpoint load failed ({e}); building a new model', level='WARN')\n",
    "\n",
    "if model is None:\n",
    "    # Build fresh model matching current data shape\n",
    "    input_shape = tuple(int(d) for d in X_train.shape[1:])\n",
    "    model = build_unet_audio_classifier(input_shape, num_classes)\n",
    "    model.compile(optimizer=keras.optimizers.Adam(1e-3), loss=keras.losses.CategoricalCrossentropy(label_smoothing=FMA_LABEL_SMOOTH), metrics=['accuracy'])\n",
    "\n",
    "# Align data tensors to model's expected input shape\n",
    "expected = model.input_shape  # (None, M, T, C)\n",
    "M_tgt = int(expected[1]) if expected[1] is not None else X_train.shape[1]\n",
    "T_tgt = int(expected[2]) if expected[2] is not None else X_train.shape[2]\n",
    "C_tgt = int(expected[3]) if expected[3] is not None else X_train.shape[3]\n",
    "\n",
    "M_cur, T_cur, C_cur = X_train.shape[1], X_train.shape[2], X_train.shape[3] if X_train.ndim == 4 else 1\n",
    "if (M_cur, T_cur, C_cur) != (M_tgt, T_tgt, C_tgt):\n",
    "    log(f'Model expects (M,T,C)=({M_tgt},{T_tgt},{C_tgt}); data has ({M_cur},{T_cur},{C_cur}). Adjusting...', level='WARN')\n",
    "    # Ensure channel dim\n",
    "    if X_train.ndim == 3:\n",
    "        X_train = X_train[..., None]\n",
    "        X_val = X_val[..., None]\n",
    "        X_test = X_test[..., None]\n",
    "    # Adjust mel bins and time frames\n",
    "    if X_train.shape[1] != M_tgt:\n",
    "        X_train = _pad_or_crop_axis(X_train, axis=1, target=M_tgt)\n",
    "        X_val   = _pad_or_crop_axis(X_val,   axis=1, target=M_tgt)\n",
    "        X_test  = _pad_or_crop_axis(X_test,  axis=1, target=M_tgt)\n",
    "    if X_train.shape[2] != T_tgt:\n",
    "        X_train = _pad_or_crop_axis(X_train, axis=2, target=T_tgt)\n",
    "        X_val   = _pad_or_crop_axis(X_val,   axis=2, target=T_tgt)\n",
    "        X_test  = _pad_or_crop_axis(X_test,  axis=2, target=T_tgt)\n",
    "    log(f'Adjusted shapes: train={X_train.shape}, val={X_val.shape}, test={X_test.shape}')\n",
    "\n",
    "# Build tf.data pipelines after finalizing shapes\n",
    "train_ds = ds_with_optional_aug(X_train, y_train_cat, BATCH_SIZE, training=True)\n",
    "val_ds   = ds_with_optional_aug(X_val,   y_val_cat, BATCH_SIZE, training=False)\n",
    "test_ds  = ds_with_optional_aug(X_test,  y_test_cat, BATCH_SIZE, training=False)\n",
    "\n",
    "# Optional class weighting to handle class imbalance\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "classes_idx = np.arange(num_classes)\n",
    "class_weights_vec = compute_class_weight(class_weight='balanced', classes=classes_idx, y=np.asarray(y_train))\n",
    "CLASS_WEIGHT = {int(i): float(w) for i, w in zip(classes_idx, class_weights_vec)}\n",
    "log(f'Class weights: {CLASS_WEIGHT}')\n",
    "\n",
    "cb = [\n",
    "    callbacks.EarlyStopping(monitor='val_accuracy', patience=FMA_PATIENCE, restore_best_weights=True),\n",
    "    callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=max(5, FMA_PATIENCE//2)),\n",
    "    callbacks.ModelCheckpoint(ckpt_path, monitor='val_accuracy', save_best_only=True)\n",
    "]\n",
    "\n",
    "# Decide training path\n",
    "history = None\n",
    "if FMA_FORCE_RETRAIN:\n",
    "    log('FORCE_RETRAIN=1 → training from scratch, ignoring checkpoint')\n",
    "    history = model.fit(train_ds, validation_data=val_ds, epochs=FMA_EPOCHS, verbose=1, callbacks=cb, class_weight=CLASS_WEIGHT)\n",
    "elif FMA_RESUME_FROM_CKPT and loaded_ckpt:\n",
    "    log('RESUME_FROM_CKPT=1 → continuing training from checkpoint')\n",
    "    history = model.fit(train_ds, validation_data=val_ds, epochs=FMA_EPOCHS, verbose=1, callbacks=cb, class_weight=CLASS_WEIGHT)\n",
    "elif SKIP_TRAIN_IF_CKPT and loaded_ckpt:\n",
    "    log('Training skipped due to existing checkpoint.')\n",
    "else:\n",
    "    log('No checkpoint scenario → training from scratch')\n",
    "    history = model.fit(train_ds, validation_data=val_ds, epochs=FMA_EPOCHS, verbose=1, callbacks=cb, class_weight=CLASS_WEIGHT)\n",
    "\n",
    "log('Evaluating on test set...')\n",
    "\n",
    "test_loss, test_acc = model.evaluate(test_ds, verbose=0)\n",
    "log(f'FMA Test Accuracy: {test_acc}')\n",
    "\n",
    "best_val = float(np.max(history.history.get('val_accuracy', [0]))) if history is not None else None\n",
    "\n",
    "pd.DataFrame([\n",
    "  {\n",
    "    'Model':'UNet_Audio_Classifier', 'Dataset':'FMA_SMALL',\n",
    "    'Best_Val_Accuracy': best_val,\n",
    "    'Test_Accuracy': float(test_acc), 'Epochs_Run': int(len(history.history.get('val_accuracy', []))) if history is not None else 0\n",
    "  }\n",
    "]).to_csv(REPORTS/'training_summary_FMA.csv', index=False)\n",
    "log(f'Saved: {REPORTS/\"training_summary_FMA.csv\"}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ec60ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:18:09] INFO: Saved: /home/alepot55/Desktop/projects/naml_project/reports/classification_report_UNet_Audio_Classifier_FMA.txt\n",
      "[19:18:09] INFO: Saved: /home/alepot55/Desktop/projects/naml_project/reports/confusion_matrix_UNet_Audio_Classifier_FMA.png\n"
     ]
    }
   ],
   "source": [
    "# After training: save classification report and confusion matrix\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get predictions\n",
    "y_pred = model.predict(test_ds, verbose=0)\n",
    "y_pred_idx = np.argmax(y_pred, axis=1)\n",
    "y_true_idx = np.argmax(y_test_cat, axis=1)\n",
    "\n",
    "# Classification report\n",
    "report = classification_report(y_true_idx, y_pred_idx, target_names=le.classes_.tolist(), zero_division=0)\n",
    "with open(REPORTS/'classification_report_UNet_Audio_Classifier_FMA.txt', 'w') as f:\n",
    "    f.write(report)\n",
    "log(f'Saved: {REPORTS/\"classification_report_UNet_Audio_Classifier_FMA.txt\"}')\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_true_idx, y_pred_idx)\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=le.classes_.tolist(), yticklabels=le.classes_.tolist())\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix — UNet_Audio_Classifier (FMA)')\n",
    "plt.tight_layout()\n",
    "fig_path = REPORTS/'confusion_matrix_UNet_Audio_Classifier_FMA.png'\n",
    "plt.savefig(fig_path)\n",
    "plt.close()\n",
    "log(f'Saved: {fig_path}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
