{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8e58de6",
   "metadata": {},
   "source": [
    "# 01 â€” Model Training Tournament (GTZAN)\n",
    "\n",
    "Esegue il torneo su 3 architetture e implementa WP1 (ablation augmentation) e WP2 (efficienza).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6092d63e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: could not set memory growth: Physical devices cannot be modified after being initialized\n",
      "Mixed precision enabled: mixed_float16\n",
      "Shapes: (6000, 128, 128, 1) (2000, 128, 128, 1) (2000, 128, 128, 1) | classes: 10\n"
     ]
    }
   ],
   "source": [
    "# Setup & data load\n",
    "import os, pickle, time, random, numpy as np, pandas as pd, tensorflow as tf\n",
    "import keras\n",
    "from keras import layers, models, callbacks\n",
    "from keras.utils import to_categorical\n",
    "from pathlib import Path\n",
    "\n",
    "# Lightweight logger\n",
    "VERBOSE = int(os.getenv('GTZAN_VERBOSE', '1'))\n",
    "\n",
    "def log(msg: str, level: str = 'INFO'):\n",
    "    if VERBOSE:\n",
    "        ts = time.strftime('%H:%M:%S')\n",
    "        print(f\"[{ts}] {level}: {msg}\")\n",
    "\n",
    "# --- Reproducibility ---\n",
    "RANDOM_STATE = 42\n",
    "random.seed(RANDOM_STATE)\n",
    "np.random.seed(RANDOM_STATE)\n",
    "tf.random.set_seed(RANDOM_STATE)\n",
    "\n",
    "# --- GPU config & Mixed Precision ---\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        log(f\"GPU(s) detected: {[tf.config.experimental.get_device_details(g)['device_name'] for g in gpus]}\")\n",
    "    except Exception as e:\n",
    "        log(f\"Could not set memory growth: {e}\", level='WARN')\n",
    "try:\n",
    "    policy = keras.mixed_precision.Policy('mixed_float16')\n",
    "    keras.mixed_precision.set_global_policy(policy)\n",
    "    log(f\"Mixed precision enabled: {keras.mixed_precision.global_policy().name}\")\n",
    "except Exception as e:\n",
    "    log(f\"Could not enable mixed precision: {e}\", level='WARN')\n",
    "\n",
    "PROJECT_ROOT = Path(os.getcwd()).resolve().parents[1]\n",
    "PROCESSED = PROJECT_ROOT/'data'/'processed'\n",
    "MODELS = PROJECT_ROOT/'models'\n",
    "REPORTS = PROJECT_ROOT/'reports'\n",
    "MODELS.mkdir(exist_ok=True); REPORTS.mkdir(exist_ok=True)\n",
    "\n",
    "X_train = np.load(PROCESSED/'X_train.npy'); y_train = np.load(PROCESSED/'y_train.npy')\n",
    "X_val = np.load(PROCESSED/'X_val.npy'); y_val = np.load(PROCESSED/'y_val.npy')\n",
    "X_test = np.load(PROCESSED/'X_test.npy'); y_test = np.load(PROCESSED/'y_test.npy')\n",
    "# Ensure float32 inputs for efficient training\n",
    "X_train = X_train.astype(np.float32)\n",
    "X_val = X_val.astype(np.float32)\n",
    "X_test = X_test.astype(np.float32)\n",
    "\n",
    "with open(PROCESSED/'label_encoder.pkl','rb') as f: le = pickle.load(f)\n",
    "num_classes = len(le.classes_)\n",
    "\n",
    "y_train_cat = to_categorical(y_train, num_classes)\n",
    "y_val_cat = to_categorical(y_val, num_classes)\n",
    "y_test_cat = to_categorical(y_test, num_classes)\n",
    "\n",
    "log(f\"Shapes: train={X_train.shape} val={X_val.shape} test={X_test.shape} | classes: {num_classes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab67fd68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SpecAugment toggleable\n",
    "@tf.function\n",
    "def spec_augment_tf(s, y):\n",
    "    \"\"\"\n",
    "    Apply SpecAugment-style frequency and time masking to a single spectrogram.\n",
    "\n",
    "    @param s: Spectrogram tensor with shape [freq_bins, time_steps, channels].\n",
    "    @param y: One-hot label tensor corresponding to s.\n",
    "    @return: Tuple (augmented_s, y).\n",
    "    \"\"\"\n",
    "    # Avoid direct tensor indexing on shapes; use tf.shape with tf.gather.\n",
    "    s_shape = tf.shape(s)\n",
    "    freq_bins = tf.gather(s_shape, 0)\n",
    "    time_steps = tf.gather(s_shape, 1)\n",
    "\n",
    "    # Frequency mask\n",
    "    max_f = tf.maximum(\n",
    "        tf.constant(2, dtype=tf.int32),\n",
    "        tf.cast(0.2 * tf.cast(freq_bins, tf.float32), tf.int32),\n",
    "    )\n",
    "    f = tf.random.uniform([], minval=1, maxval=max_f, dtype=tf.int32)\n",
    "    max_f0 = tf.maximum(tf.constant(1, dtype=tf.int32), freq_bins - f)\n",
    "    f0 = tf.random.uniform([], minval=0, maxval=max_f0, dtype=tf.int32)\n",
    "    mask_f = tf.concat(\n",
    "        [\n",
    "            tf.ones([f0], dtype=s.dtype),\n",
    "            tf.zeros([f], dtype=s.dtype),\n",
    "            tf.ones([freq_bins - f0 - f], dtype=s.dtype),\n",
    "        ],\n",
    "        axis=0,\n",
    "    )\n",
    "    s = s * tf.reshape(mask_f, [freq_bins, 1, 1])\n",
    "\n",
    "    # Time mask\n",
    "    max_t = tf.maximum(\n",
    "        tf.constant(2, dtype=tf.int32),\n",
    "        tf.cast(0.2 * tf.cast(time_steps, tf.float32), tf.int32),\n",
    "    )\n",
    "    t = tf.random.uniform([], minval=1, maxval=max_t, dtype=tf.int32)\n",
    "    max_t0 = tf.maximum(tf.constant(1, dtype=tf.int32), time_steps - t)\n",
    "    t0 = tf.random.uniform([], minval=0, maxval=max_t0, dtype=tf.int32)\n",
    "    mask_t = tf.concat(\n",
    "        [\n",
    "            tf.ones([t0], dtype=s.dtype),\n",
    "            tf.zeros([t], dtype=s.dtype),\n",
    "            tf.ones([time_steps - t0 - t], dtype=s.dtype),\n",
    "        ],\n",
    "        axis=0,\n",
    "    )\n",
    "    s = s * tf.reshape(mask_t, [1, time_steps, 1])\n",
    "\n",
    "    return s, y\n",
    "\n",
    "\n",
    "def make_pipelines(use_aug: bool, batch=48):\n",
    "    \"\"\"\n",
    "    Build tf.data pipelines for training, validation, and test sets.\n",
    "\n",
    "    @param use_aug: If True, apply SpecAugment on-the-fly to the training set.\n",
    "    @param batch: Batch size for all datasets.\n",
    "    @return: Tuple (train_ds, val_ds, test_ds).\n",
    "    \"\"\"\n",
    "    AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "    # Training dataset: shuffle deterministically and apply augmentation only if requested.\n",
    "    ds_train = (\n",
    "        tf.data.Dataset.from_tensor_slices((X_train, y_train_cat))\n",
    "        .shuffle(buffer_size=len(X_train), seed=RANDOM_STATE, reshuffle_each_iteration=True)\n",
    "    )\n",
    "    if use_aug:\n",
    "        ds_train = ds_train.map(spec_augment_tf, num_parallel_calls=AUTOTUNE)\n",
    "    ds_train = ds_train.batch(batch, drop_remainder=True).prefetch(AUTOTUNE)\n",
    "\n",
    "    # Validation/Test: cache and prefetch for speed. No augmentation.\n",
    "    ds_val = (\n",
    "        tf.data.Dataset.from_tensor_slices((X_val, y_val_cat))\n",
    "        .batch(batch)\n",
    "        .cache()\n",
    "        .prefetch(AUTOTUNE)\n",
    "    )\n",
    "    ds_test = (\n",
    "        tf.data.Dataset.from_tensor_slices((X_test, y_test_cat))\n",
    "        .batch(batch)\n",
    "        .cache()\n",
    "        .prefetch(AUTOTUNE)\n",
    "    )\n",
    "    return ds_train, ds_val, ds_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7992d103",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Curated ModelFactory (aligned with original curated definitions)\n",
    "from keras import layers, models\n",
    "\n",
    "class ModelFactory:\n",
    "    \"\"\"\n",
    "    A curated factory for building and comparing the three key CNN\n",
    "    architectures selected for our final, focused analysis.\n",
    "    \"\"\"\n",
    "\n",
    "    # -------------------------- Helper blocks ---------------------------\n",
    "    @staticmethod\n",
    "    def _se_block(input_tensor, ratio=8, name_prefix=\"\"):\n",
    "        \"\"\"Squeeze-and-Excitation block to add channel-wise attention.\"\"\"\n",
    "        channels = input_tensor.shape[-1]\n",
    "        se = layers.GlobalAveragePooling2D(name=f'{name_prefix}_se_squeeze')(input_tensor)\n",
    "        se = layers.Reshape((1, 1, channels))(se)\n",
    "        se = layers.Dense(channels // ratio, activation='relu', name=f'{name_prefix}_se_excite_1')(se)\n",
    "        se = layers.Dense(channels, activation='sigmoid', name=f'{name_prefix}_se_excite_2')(se)\n",
    "        return layers.Multiply(name=f'{name_prefix}_se_scale')([input_tensor, se])\n",
    "\n",
    "    # --------------------------- Efficient VGG --------------------------\n",
    "    @staticmethod\n",
    "    def build_efficient_vgg(input_shape, num_classes):\n",
    "        inputs = layers.Input(shape=input_shape)\n",
    "        # Block 1\n",
    "        x = layers.Conv2D(16, 3, padding='same', use_bias=False)(inputs)\n",
    "        x = layers.BatchNormalization()(x); x = layers.Activation('relu')(x)\n",
    "        x = layers.MaxPooling2D(2)(x)\n",
    "        x = ModelFactory._se_block(x, name_prefix=\"vgg_b1\")\n",
    "        # Block 2\n",
    "        x = layers.Conv2D(32, 3, padding='same', use_bias=False)(x)\n",
    "        x = layers.BatchNormalization()(x); x = layers.Activation('relu')(x)\n",
    "        x = layers.MaxPooling2D(2)(x)\n",
    "        x = ModelFactory._se_block(x, name_prefix=\"vgg_b2\")\n",
    "        # Block 3\n",
    "        x = layers.Conv2D(64, 3, padding='same', use_bias=False)(x)\n",
    "        x = layers.BatchNormalization()(x); x = layers.Activation('relu')(x)\n",
    "        x = layers.MaxPooling2D(2)(x)\n",
    "        x = ModelFactory._se_block(x, name_prefix=\"vgg_b3\")\n",
    "        # Head\n",
    "        x = layers.GlobalAveragePooling2D(name=\"gap\")(x)\n",
    "        x = layers.Dense(128, activation='relu')(x)\n",
    "        x = layers.Dropout(0.5)(x)\n",
    "        outputs = layers.Dense(num_classes, activation='softmax', dtype='float32')(x)\n",
    "        return models.Model(inputs=inputs, outputs=outputs, name='Efficient_VGG')\n",
    "\n",
    "    # --------------------------- ResSE AudioCNN -------------------------\n",
    "    @staticmethod\n",
    "    def _res_se_block(input_tensor, filters, stride=1, name_prefix=\"\"):\n",
    "        shortcut = input_tensor\n",
    "        x = layers.Conv2D(filters, 3, strides=stride, padding='same', use_bias=False, name=f'{name_prefix}_conv1')(input_tensor)\n",
    "        x = layers.BatchNormalization(name=f'{name_prefix}_bn1')(x)\n",
    "        x = layers.PReLU(shared_axes=[1, 2], name=f'{name_prefix}_prelu1')(x)\n",
    "        x = layers.Conv2D(filters, 3, padding='same', use_bias=False, name=f'{name_prefix}_conv2')(x)\n",
    "        x = layers.BatchNormalization(name=f'{name_prefix}_bn2')(x)\n",
    "        x = ModelFactory._se_block(x, name_prefix=f'{name_prefix}_se')\n",
    "        if stride > 1 or shortcut.shape[-1] != filters:\n",
    "            shortcut = layers.Conv2D(filters, 1, strides=stride, use_bias=False, name=f'{name_prefix}_shortcut_conv')(shortcut)\n",
    "            shortcut = layers.BatchNormalization(name=f'{name_prefix}_shortcut_bn')(shortcut)\n",
    "        x = layers.Add(name=f'{name_prefix}_add')([shortcut, x])\n",
    "        x = layers.PReLU(shared_axes=[1, 2], name=f'{name_prefix}_prelu2')(x)\n",
    "        return x\n",
    "\n",
    "    @staticmethod\n",
    "    def build_res_se_audio_cnn(input_shape, num_classes):\n",
    "        inputs = layers.Input(shape=input_shape)\n",
    "        x = layers.Conv2D(32, 3, strides=1, padding='same', use_bias=False)(inputs)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.PReLU(shared_axes=[1, 2])(x)\n",
    "        x = ModelFactory._res_se_block(x, 64, stride=2, name_prefix=\"res_b1\")\n",
    "        x = ModelFactory._res_se_block(x, 128, stride=2, name_prefix=\"res_b2\")\n",
    "        x = ModelFactory._res_se_block(x, 256, stride=2, name_prefix=\"res_b3\")\n",
    "        x = layers.GlobalAveragePooling2D(name=\"gap\")(x)\n",
    "        x = layers.Dropout(0.5)(x)\n",
    "        outputs = layers.Dense(num_classes, activation='softmax', dtype='float32')(x)\n",
    "        return models.Model(inputs=inputs, outputs=outputs, name='ResSE_AudioCNN')\n",
    "\n",
    "    # --------------------------- UNet Audio Classifier ------------------\n",
    "    @staticmethod\n",
    "    def _unet_encoder_block(input_tensor, filters, pool=True, name_prefix=\"\"):\n",
    "        x = layers.Conv2D(filters, 3, padding='same', use_bias=False, name=f'{name_prefix}_conv1')(input_tensor)\n",
    "        x = layers.BatchNormalization(name=f'{name_prefix}_bn1')(x); x = layers.PReLU(shared_axes=[1, 2], name=f'{name_prefix}_prelu1')(x)\n",
    "        x = layers.Conv2D(filters, 3, padding='same', use_bias=False, name=f'{name_prefix}_conv2')(x)\n",
    "        x = layers.BatchNormalization(name=f'{name_prefix}_bn2')(x); x = layers.PReLU(shared_axes=[1, 2], name=f'{name_prefix}_prelu2')(x)\n",
    "        skip_connection = x\n",
    "        if pool:\n",
    "            pool_output = layers.MaxPooling2D(2, name=f'{name_prefix}_pool')(x)\n",
    "            return pool_output, skip_connection\n",
    "        else:\n",
    "            return x, skip_connection\n",
    "\n",
    "    @staticmethod\n",
    "    def build_unet_audio_classifier(input_shape, num_classes):\n",
    "        inputs = layers.Input(shape=input_shape)\n",
    "        # Encoder path\n",
    "        p1, s1 = ModelFactory._unet_encoder_block(inputs, 32, name_prefix=\"enc1\")\n",
    "        p2, s2 = ModelFactory._unet_encoder_block(p1, 64, name_prefix=\"enc2\")\n",
    "        p3, s3 = ModelFactory._unet_encoder_block(p2, 128, name_prefix=\"enc3\")\n",
    "        # Bottleneck (pool=False)\n",
    "        bottleneck, _ = ModelFactory._unet_encoder_block(p3, 256, pool=False, name_prefix=\"bneck\")\n",
    "        # Classification head\n",
    "        x = layers.GlobalAveragePooling2D(name=\"gap\")(bottleneck)\n",
    "        x = layers.Dropout(0.5)(x)\n",
    "        outputs = layers.Dense(num_classes, activation='softmax', dtype='float32')(x)\n",
    "        return models.Model(inputs=inputs, outputs=outputs, name='UNet_Audio_Classifier')\n",
    "\n",
    "# Final registry of models\n",
    "FINAL_MODELS = {\n",
    "    'Efficient_VGG': ModelFactory.build_efficient_vgg,\n",
    "    'ResSE_AudioCNN': ModelFactory.build_res_se_audio_cnn,\n",
    "    'UNet_Audio_Classifier': ModelFactory.build_unet_audio_classifier,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d90006",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training/eval orchestration with efficiency metrics (WP2)\n",
    "\n",
    "def count_params(model):\n",
    "    return np.sum([np.prod(v.shape) for v in model.trainable_variables])\n",
    "\n",
    "\n",
    "def measure_latency(model, sample, runs=50, warmup=5):\n",
    "    x = tf.convert_to_tensor(sample)\n",
    "    for _ in range(warmup):\n",
    "        _ = model(x, training=False)\n",
    "    start = time.time()\n",
    "    for _ in range(runs):\n",
    "        _ = model(x, training=False)\n",
    "    end = time.time()\n",
    "    return (end - start) / runs * 1000.0\n",
    "\n",
    "\n",
    "def approximate_flops(model):\n",
    "    \"\"\"\n",
    "    Rough FLOPs estimator (multiply-adds) for common layers.\n",
    "    - Conv2D: H*W*C_in*C_out*K*K*2\n",
    "    - DepthwiseConv2D: H*W*C_out*K*K*2\n",
    "    - Dense: in*out*2\n",
    "    Returns total FLOPs for a single forward pass on batch size 1.\n",
    "    \"\"\"\n",
    "    total = 0\n",
    "    from keras.layers import Conv2D, DepthwiseConv2D, Dense, Conv2DTranspose\n",
    "\n",
    "    for layer in model.layers:\n",
    "        try:\n",
    "            out_shape = layer.output_shape\n",
    "        except Exception:\n",
    "            continue\n",
    "        if isinstance(layer, Conv2D):\n",
    "            if None in (out_shape[1], out_shape[2], out_shape[3]):\n",
    "                continue\n",
    "            k_h, k_w = layer.kernel_size\n",
    "            H, W, C_out = out_shape[1], out_shape[2], out_shape[3]\n",
    "            C_in = layer.input_shape[-1]\n",
    "            total += H * W * C_in * C_out * k_h * k_w * 2\n",
    "        elif isinstance(layer, DepthwiseConv2D):\n",
    "            if None in (out_shape[1], out_shape[2], out_shape[3]):\n",
    "                continue\n",
    "            k_h, k_w = layer.kernel_size\n",
    "            H, W, C_out = out_shape[1], out_shape[2], out_shape[3]\n",
    "            total += H * W * C_out * k_h * k_w * 2\n",
    "        elif isinstance(layer, Conv2DTranspose):\n",
    "            if None in (out_shape[1], out_shape[2], out_shape[3]):\n",
    "                continue\n",
    "            k_h, k_w = layer.kernel_size\n",
    "            H, W, C_out = out_shape[1], out_shape[2], out_shape[3]\n",
    "            C_in = layer.input_shape[-1]\n",
    "            total += H * W * C_in * C_out * k_h * k_w * 2\n",
    "        elif isinstance(layer, Dense):\n",
    "            try:\n",
    "                in_units = layer.input_shape[-1]\n",
    "                out_units = layer.units\n",
    "                if None not in (in_units, out_units):\n",
    "                    total += in_units * out_units * 2\n",
    "            except Exception:\n",
    "                pass\n",
    "    return float(total)\n",
    "\n",
    "\n",
    "def run_tournament(use_aug: bool, tag: str, epochs=120, patience=25, batch=48):\n",
    "    train_ds, val_ds, test_ds = make_pipelines(use_aug, batch)\n",
    "    input_shape = X_train.shape[1:]\n",
    "    results = []\n",
    "    for name, builder in FINAL_MODELS.items():\n",
    "        keras.backend.clear_session()\n",
    "        model = builder(input_shape, num_classes)\n",
    "        model.compile(optimizer=keras.optimizers.Adam(1e-3), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "        ckpt_path = MODELS / f\"{name}_best_{tag}.keras\"\n",
    "        cb = [\n",
    "            callbacks.EarlyStopping(monitor='val_accuracy', patience=patience, restore_best_weights=True),\n",
    "            callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=max(2, patience // 3)),\n",
    "            callbacks.ModelCheckpoint(ckpt_path, monitor='val_accuracy', save_best_only=True),\n",
    "        ]\n",
    "        log(f\"Training {name} [{tag}] for up to {epochs} epochs...\")\n",
    "        h = model.fit(train_ds, validation_data=val_ds, epochs=epochs, verbose=0, callbacks=cb)\n",
    "        test_loss, test_acc = model.evaluate(test_ds, verbose=0)\n",
    "        params = count_params(model)\n",
    "        sample = X_test[:1]\n",
    "        latency_ms = measure_latency(model, sample)\n",
    "        flops = approximate_flops(model)\n",
    "        best_val = float(np.max(h.history.get('val_accuracy', [0])))\n",
    "        log(f\"{name} [{tag}] -> Best Val: {best_val:.4f} | Test: {float(test_acc):.4f} | Params: {int(params)} | Latency(ms): {latency_ms:.2f}\")\n",
    "        results.append({\n",
    "            'Model': name,\n",
    "            'Tag': tag,\n",
    "            'Best_Val_Accuracy': best_val,\n",
    "            'Test_Accuracy': float(test_acc),\n",
    "            'Epochs_Run': int(len(h.history.get('val_accuracy', []))),\n",
    "            'Params': int(params),\n",
    "            'Approx_FLOPs': float(flops) if np.isfinite(flops) else None,\n",
    "            'Latency_ms': float(latency_ms),\n",
    "        })\n",
    "    df = pd.DataFrame(results)\n",
    "    out_csv = REPORTS / f\"training_summary_{tag}.csv\"\n",
    "    df.to_csv(out_csv, index=False)\n",
    "    log(f'Saved: {out_csv}')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a178016",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Efficient_VGG [WITH_AUG] for up to 120 epochs...\n",
      "Efficient_VGG [WITH_AUG] -> Best Val: 0.7530 | Test: 0.7520\n",
      "Efficient_VGG [WITH_AUG] -> Best Val: 0.7530 | Test: 0.7520\n",
      "\n",
      "Training ResSE_AudioCNN [WITH_AUG] for up to 120 epochs...\n",
      "\n",
      "Training ResSE_AudioCNN [WITH_AUG] for up to 120 epochs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-17 11:05:43.930991: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_5', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n",
      "2025-08-17 11:05:52.909217: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_5', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n",
      "2025-08-17 11:05:52.909217: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_5', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResSE_AudioCNN [WITH_AUG] -> Best Val: 0.8155 | Test: 0.7980\n",
      "\n",
      "Training UNet_Audio_Classifier [WITH_AUG] for up to 120 epochs...\n",
      "\n",
      "Training UNet_Audio_Classifier [WITH_AUG] for up to 120 epochs...\n",
      "UNet_Audio_Classifier [WITH_AUG] -> Best Val: 0.8525 | Test: 0.8230\n",
      "Saved: /home/alepot55/Desktop/projects/naml_project/reports/training_summary_WITH_AUG.csv\n",
      "UNet_Audio_Classifier [WITH_AUG] -> Best Val: 0.8525 | Test: 0.8230\n",
      "Saved: /home/alepot55/Desktop/projects/naml_project/reports/training_summary_WITH_AUG.csv\n",
      "\n",
      "Training Efficient_VGG [NO_AUG] for up to 120 epochs...\n",
      "\n",
      "Training Efficient_VGG [NO_AUG] for up to 120 epochs...\n",
      "Efficient_VGG [NO_AUG] -> Best Val: 0.7600 | Test: 0.7535\n",
      "\n",
      "Training ResSE_AudioCNN [NO_AUG] for up to 120 epochs...\n",
      "Efficient_VGG [NO_AUG] -> Best Val: 0.7600 | Test: 0.7535\n",
      "\n",
      "Training ResSE_AudioCNN [NO_AUG] for up to 120 epochs...\n",
      "ResSE_AudioCNN [NO_AUG] -> Best Val: 0.8195 | Test: 0.7990\n",
      "ResSE_AudioCNN [NO_AUG] -> Best Val: 0.8195 | Test: 0.7990\n",
      "\n",
      "Training UNet_Audio_Classifier [NO_AUG] for up to 120 epochs...\n",
      "\n",
      "Training UNet_Audio_Classifier [NO_AUG] for up to 120 epochs...\n",
      "UNet_Audio_Classifier [NO_AUG] -> Best Val: 0.8440 | Test: 0.8180\n",
      "Saved: /home/alepot55/Desktop/projects/naml_project/reports/training_summary_NO_AUG.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Tag</th>\n",
       "      <th>Best_Val_Accuracy</th>\n",
       "      <th>Test_Accuracy</th>\n",
       "      <th>Epochs_Run</th>\n",
       "      <th>Params</th>\n",
       "      <th>Approx_FLOPs</th>\n",
       "      <th>Latency_ms</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Efficient_VGG</td>\n",
       "      <td>WITH_AUG</td>\n",
       "      <td>0.7530</td>\n",
       "      <td>0.7520</td>\n",
       "      <td>96</td>\n",
       "      <td>34488</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.771867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ResSE_AudioCNN</td>\n",
       "      <td>WITH_AUG</td>\n",
       "      <td>0.8155</td>\n",
       "      <td>0.7980</td>\n",
       "      <td>55</td>\n",
       "      <td>1232770</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26.752510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>UNet_Audio_Classifier</td>\n",
       "      <td>WITH_AUG</td>\n",
       "      <td>0.8525</td>\n",
       "      <td>0.8230</td>\n",
       "      <td>73</td>\n",
       "      <td>1176170</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23.066792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Efficient_VGG</td>\n",
       "      <td>NO_AUG</td>\n",
       "      <td>0.7600</td>\n",
       "      <td>0.7535</td>\n",
       "      <td>81</td>\n",
       "      <td>34488</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.259983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ResSE_AudioCNN</td>\n",
       "      <td>NO_AUG</td>\n",
       "      <td>0.8195</td>\n",
       "      <td>0.7990</td>\n",
       "      <td>61</td>\n",
       "      <td>1232770</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21.596742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>UNet_Audio_Classifier</td>\n",
       "      <td>NO_AUG</td>\n",
       "      <td>0.8440</td>\n",
       "      <td>0.8180</td>\n",
       "      <td>63</td>\n",
       "      <td>1176170</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.750150</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Model       Tag  Best_Val_Accuracy  Test_Accuracy  \\\n",
       "0          Efficient_VGG  WITH_AUG             0.7530         0.7520   \n",
       "1         ResSE_AudioCNN  WITH_AUG             0.8155         0.7980   \n",
       "2  UNet_Audio_Classifier  WITH_AUG             0.8525         0.8230   \n",
       "3          Efficient_VGG    NO_AUG             0.7600         0.7535   \n",
       "4         ResSE_AudioCNN    NO_AUG             0.8195         0.7990   \n",
       "5  UNet_Audio_Classifier    NO_AUG             0.8440         0.8180   \n",
       "\n",
       "   Epochs_Run   Params  Approx_FLOPs  Latency_ms  \n",
       "0          96    34488           0.0   14.771867  \n",
       "1          55  1232770           0.0   26.752510  \n",
       "2          73  1176170           0.0   23.066792  \n",
       "3          81    34488           0.0   18.259983  \n",
       "4          61  1232770           0.0   21.596742  \n",
       "5          63  1176170           0.0   11.750150  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run WP1: WITH_AUG and NO_AUG, and display WP2 metrics\n",
    "df_with = run_tournament(use_aug=True, tag='WITH_AUG')\n",
    "df_no = run_tournament(use_aug=False, tag='NO_AUG')\n",
    "\n",
    "from IPython.display import display\n",
    "display(pd.concat([df_with, df_no], ignore_index=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
